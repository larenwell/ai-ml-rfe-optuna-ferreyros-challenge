{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dkxDYEH6aS8V",
        "yBvcs74YaN5K",
        "uGiAdr7RjBWM",
        "4IBy1riujdGc",
        "RSRsAww2g47A"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATATHON FERREYROS\n",
        "\n",
        "En Ferreyros no solo vendemos máquinas, también nos aseguramos de maximizar la productividad de nuestros clientes y la vida útil de las máquinas, ofreciendo repuestos y servicios de manera proactiva y oportuna.\n",
        "\n",
        "Por ello, priorizar las oportunidades de venta de repuestos y servicios nos permite brindar el mejor servicio a los clientes que más lo necesitan.\n",
        "\n",
        "Tu misión consiste en utilizar la data histórica de oportunidades para estimar la probabilidad de cierre de nuevas oportunidades. Adicionalmente, tendrás acceso a data del parque de máquinas y horómetros de las máquinas."
      ],
      "metadata": {
        "id": "8UqcvEmmjzSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerías\n"
      ],
      "metadata": {
        "id": "dkxDYEH6aS8V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HOvBWRnZhuP"
      },
      "outputs": [],
      "source": [
        "# Cargar Librerías\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import sys\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición de funciones"
      ],
      "metadata": {
        "id": "yBvcs74YaN5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de funciones\n",
        "\n",
        "\n",
        "def display_table_info_and_head(table, table_name):\n",
        "    print(\"Información de la tabla:\",table_name,\"\\n\")\n",
        "    print(table.info(),\"\\n\")\n",
        "    print(\"Mostrar los primeros 5 registros:\",\"\\n\")\n",
        "    print(table.head(5))\n",
        "\n",
        "def get_items_by_opportunity(opportunity_id):\n",
        "    items = OpportunityItemPostVenta[OpportunityItemPostVenta['OpportunityID'] == opportunity_id]\n",
        "    return items\n",
        "​\n",
        "def get_closed_won_ppportunity(opportunity_id):\n",
        "    items = X_train_df[X_train_df['OpportunityID'] == opportunity_id]\n",
        "    return items\n",
        "​\n",
        "def get_customer_id(customer_id):\n",
        "    items = EquipmentPostVenta[EquipmentPostVenta['CurrentCustomerID'] == customer_id]\n",
        "    return items\n",
        "​\n",
        "def get_serial_number_id(serial_id):\n",
        "    items = horometros_vl[horometros_vl['SerialNumber'] == serial_id]\n",
        "    return items\n",
        "​\n",
        "def fx_porc_missings(data, only_missings = False):\n",
        "    df_vars_missings = pd.concat([pd.DataFrame(data.isnull().sum(), columns = ['n_nulos']),\n",
        "           pd.DataFrame(100*data.isnull().sum()/len(data), columns = ['%Total'])], axis = 1)\n",
        "    if only_missings:\n",
        "        return(df_vars_missings[df_vars_missings[\"n_nulos\"]!=0])\n",
        "    else:\n",
        "        return(df_vars_missings)\n",
        "\n",
        "def plot_graph_univariate(df, meta):\n",
        "    import matplotlib\n",
        "    matplotlib.rcParams.update({'font.size': 14})\n",
        "    for i in range(len(meta)):\n",
        "        plt.figure(figsize=(55,8))\n",
        "        v=meta.iloc[i].variable\n",
        "        t=meta.iloc[i].tipo\n",
        "        if (t.__class__.__name__==\"CategoricalDtype\"):\n",
        "            fa=df[v].value_counts()\n",
        "            fr=fa/len(df[v])\n",
        "            #Barras\n",
        "            plt.subplot(1,2,1)\n",
        "            ax=plt.bar(fa.index,fa,edgecolor='black')\n",
        "            plt.xticks(fa.index,rotation=90)\n",
        "            plt.title('Distribution of '+v+' categories')\n",
        "\n",
        "            total = len(df[v])\n",
        "            max_height=fa.max()\n",
        "            for p in ax:\n",
        "                percentage = f'{100 * p.get_height() / total:.1f}%'\n",
        "                x = p.get_x() + p.get_width() / 2\n",
        "                y = p.get_height()\n",
        "                plt.annotate(percentage, (x, y + 0.007*total), ha='center', va='bottom', fontsize=12, rotation=90)\n",
        "            plt.ylim(0, max_height * 1.3)\n",
        "            plt.grid(False)\n",
        "\n",
        "            # Pie\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.pie(fr, labels=fr.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
        "            plt.legend(fr.index, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
        "            plt.title('Pie chart of ' + v)\n",
        "            plt.grid(False)\n",
        "        else:\n",
        "            #Histograma\n",
        "            plt.subplot(1,2,1)\n",
        "            plt.hist(df[v].dropna(),bins=100,edgecolor='black')\n",
        "            plt.title('Histogram of '+v)\n",
        "            #Boxplot\n",
        "            plt.subplot(1,2,2)\n",
        "            plt.boxplot(df[v],vert=False)\n",
        "            plt.title('Boxplot of '+v)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "​\n",
        "​\n",
        "def plot_graph_bivariate(df2, meta, y):\n",
        "    import matplotlib\n",
        "    matplotlib.rcParams.update({'font.size': 16})\n",
        "    for i in range(len(meta)):\n",
        "        plt.figure(figsize=(35, 10))\n",
        "        v = meta.iloc[i].variable\n",
        "        t = meta.iloc[i].tipo\n",
        "        if v == y:\n",
        "            break\n",
        "        print(v)\n",
        "        if t == \"category\":\n",
        "            g = df2.groupby([df2[y], v],observed=True).size().unstack(0)\n",
        "            tf = g[1] / (g[0] + g[1])\n",
        "            g_sorted = g.reindex(tf.sort_values(ascending=False).index)\n",
        "            c1 = g_sorted[0]\n",
        "            c2 = g_sorted[1]\n",
        "            tf_sorted = tf.sort_values(ascending=False)\n",
        "            width = 0.9\n",
        "​\n",
        "            p1 = plt.bar(g_sorted.index, c1, width)\n",
        "            p2 = plt.bar(g_sorted.index, c2, width, bottom=c1)\n",
        "​\n",
        "            plt.ylabel('Freq')\n",
        "            plt.title('Bivariate: ' + v)\n",
        "            plt.xticks(g_sorted.index, rotation=90)\n",
        "            plt.legend((p1[0], p2[0]), ('0', '1'), loc='lower left', bbox_to_anchor=(1, 1))\n",
        "​\n",
        "            tf_line = plt.twinx().plot(tf_sorted.values, linestyle='-', linewidth=3.0, color='black', marker=\"o\")\n",
        "            plt.ylabel(y)\n",
        "\n",
        "            for x, val in enumerate(tf_sorted.values):\n",
        "                plt.text(x, val+0.0003, f'{val:.2f}', color='black', fontsize=12, ha='center', va='bottom',rotation=90)\n",
        "\n",
        "        else:\n",
        "            d, bins = pd.qcut(df2[v], 10, retbins=True, duplicates='drop', labels=False)\n",
        "            g = df2.groupby([y, d]).size().unstack(0)\n",
        "            tf = g[1] / (g[0] + g[1])\n",
        "            g_sorted = g.reindex(tf.sort_values(ascending=False).index)\n",
        "            N = len(g_sorted)\n",
        "            mMeans = g_sorted[0]\n",
        "            wMeans = g_sorted[1]\n",
        "            tf_sorted = tf.sort_values(ascending=False)\n",
        "            ind = np.arange(N)\n",
        "​\n",
        "            width = 0.9\n",
        "            p1 = plt.bar(ind, mMeans, width)\n",
        "            p2 = plt.bar(ind, wMeans, width, bottom=mMeans)\n",
        "​\n",
        "            plt.ylabel('Freq')\n",
        "            plt.xlabel(\"Deciles \" + v)\n",
        "            plt.title('Bivariate: ' + v + \" vs \" + y)\n",
        "            bin_labels = [f'{bins[i]:.2f} - {bins[i+1]:.2f}' for i in range(len(bins) - 1)]\n",
        "            plt.xticks(ind, bin_labels, rotation=90)\n",
        "            plt.legend((p1[0], p2[0]), ('0', '1'), loc='lower left', bbox_to_anchor=(1, 1))\n",
        "​\n",
        "            tf_line = plt.twinx().plot(tf_sorted.values, linestyle='-', linewidth=2.0, color='black')\n",
        "            plt.ylabel(y)\n",
        "\n",
        "            for x, val in enumerate(tf_sorted.values):\n",
        "                plt.text(x, val+0.0003, f'{val:.2f}', color='black', fontsize=12, ha='center', va='bottom',rotation=90)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "​\n",
        "def create_columns_based_nulls(df,exclude_cols=None,name='field'):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = []\n",
        "\n",
        "    # Crear columnas indicadoras de valores faltantes\n",
        "    missing_cols = df.columns[df.isna().any()].tolist()\n",
        "    for col in missing_cols:\n",
        "        if col not in exclude_cols:\n",
        "            df[f'{col}_missing_{name}'] = df[col].isna().astype(int)\n",
        "\n",
        "    return df\n",
        "​\n",
        "def handle_missing_values_9999(df,fill_value=9999, exclude_cols=None):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = []\n",
        "\n",
        "    # Llenar valores faltantes con el valor extremo, excluyendo columnas especificadas\n",
        "    for col in df.columns:\n",
        "        if col not in exclude_cols:\n",
        "            df[col].fillna(fill_value, inplace=True)\n",
        "\n",
        "    return df\n",
        "​\n",
        "def handle_nans_based_on_criteria(df):\n",
        "    # Rellenar count y sum con 0 cuando NaN\n",
        "    count_sum_cols = [col for col in df.columns if 'count' in col or 'sum' in col]\n",
        "    df[count_sum_cols] = df[count_sum_cols].fillna(0)\n",
        "\n",
        "    # Rellenar std con 0 cuando solo hay un registro, y con 99999 cuando falta la categoría específica\n",
        "    std_cols = [col for col in df.columns if 'std' in col]\n",
        "    for col in std_cols:\n",
        "        df[col] = df.apply(lambda row: 0 if row.get(col.replace('std', 'count'), 0) == 1 else 99999 if pd.isna(row[col]) else row[col], axis=1)\n",
        "\n",
        "    # Rellenar otros NaNs con 99999\n",
        "    other_cols = [col for col in df.columns if col not in count_sum_cols + std_cols]\n",
        "    df[other_cols] = df[other_cols].fillna(99999)\n",
        "\n",
        "    return df\n",
        "​\n",
        "​\n",
        "def correlation_heatmap(df):\n",
        "    \"\"\"Función para plotear las correlaciones de las variables de un dataset\"\"\"\n",
        "\n",
        "    _ , ax = plt.subplots(figsize =(10, 6))\n",
        "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
        "\n",
        "    _ = sns.heatmap(\n",
        "        df.corr(),\n",
        "        cmap = colormap,\n",
        "        square=True,\n",
        "        cbar_kws={'shrink':.9 },\n",
        "        ax=ax,\n",
        "        annot=True,\n",
        "        fmt='.2f',\n",
        "        linewidths=0.1,\n",
        "        vmax=1.0,\n",
        "        vmin=-1.0,\n",
        "        linecolor='white',\n",
        "        annot_kws={'fontsize':8}\n",
        "    )\n",
        "    plt.title('Pearson Correlation of Features', y=1.05, size=12)\n",
        "    plt.show()\n",
        "\n",
        "​\n",
        "def drop_highly_correlated_features(df, threshold=0.9):\n",
        "    \"\"\"\n",
        "    Analyzes and drops highly correlated numerical features from the dataframe.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The input dataframe.\n",
        "    threshold (float): The correlation threshold to identify highly correlated features.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: The dataframe with highly correlated numerical features dropped.\n",
        "    \"\"\"\n",
        "    # Select only numerical features\n",
        "    numerical_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    corr_matrix = numerical_df.corr().abs()\n",
        "​\n",
        "    # Select the upper triangle of the correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "​\n",
        "    # Find features with correlation greater than the threshold\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "​\n",
        "    # Drop the highly correlated features from the original dataframe\n",
        "    df_dropped = df.drop(columns=to_drop)\n",
        "​\n",
        "    print(f\"Dropped columns: {to_drop}\")\n",
        "​\n",
        "    return df_dropped\n",
        "​\n",
        "# Example usage:\n",
        "# df_cleaned = drop_highly_correlated_features(df, threshold=0.9)\n",
        "​\n",
        "​\n",
        "def feature_selection_customized_initial(id_column, train_data, variance_threshold=0.005, max_missing_percentage=0.9, correlation_threshold=0.85):\n",
        "    \"\"\"\n",
        "    Perform feature selection based on variance, missing values, and correlation.\n",
        "​\n",
        "    Args:\n",
        "    - id_column (string): ID of the row\n",
        "    - train_data (pd.DataFrame): Training data.\n",
        "    - variance_threshold (float): Threshold for variance-based feature selection.\n",
        "    - max_missing_percentage (float): Maximum allowed percentage of missing values.\n",
        "    - correlation_threshold (float): Threshold for correlation-based feature selection.\n",
        "​\n",
        "    Returns:\n",
        "    - train_data (pd.DataFrame): Dataframe after feature selection.\n",
        "    \"\"\"\n",
        "    # 0. Replacing inf values\n",
        "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    cols_to_keep_vt = set(train_data.columns)\n",
        "    cols_to_keep_na = set(train_data.columns)\n",
        "    cols_to_keep_corr = set(train_data.columns)\n",
        "\n",
        "    # 1. Variance Threshold\n",
        "    if variance_threshold > 0:\n",
        "        numerical_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        numerical_cols = [col for col in numerical_cols if col != id_column]\n",
        "        selector = VarianceThreshold(threshold=variance_threshold)\n",
        "        selector.fit(train_data[numerical_cols])\n",
        "        cols_to_keep_vt = set(train_data[numerical_cols].columns[selector.get_support(indices=True)].tolist())\n",
        "​\n",
        "    # 2. Missing Values\n",
        "    if max_missing_percentage > 0:\n",
        "        missing_percentage = train_data.isnull().mean()\n",
        "        cols_to_drop_na = missing_percentage[missing_percentage > max_missing_percentage].index\n",
        "        cols_to_keep_na = set(train_data.columns) - set(cols_to_drop_na)\n",
        "​\n",
        "    train_data = train_data[list(cols_to_keep_vt.intersection(cols_to_keep_na))]\n",
        "​\n",
        "    # 3. Correlation Matrix Elimination\n",
        "    if correlation_threshold > 0:\n",
        "        # Calculate correlation matrix in chunks to avoid memory issues\n",
        "        chunk_size = 25\n",
        "        correlated_features = set()\n",
        "        for i in range(0, len(train_data.columns), chunk_size):\n",
        "            chunk = train_data.iloc[:, i:i+chunk_size]\n",
        "            correlation_matrix = chunk.corr().abs()\n",
        "            upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "            for column in upper_triangle.columns:\n",
        "                if any(upper_triangle[column] > correlation_threshold):\n",
        "                    correlated_features.add(column)\n",
        "                    cols_to_keep_corr = set(train_data.columns) - set(correlated_features)\n",
        "​\n",
        "    return cols_to_keep_corr.union(cols_to_keep_na).union(cols_to_keep_vt).union(set([id_column]))\n",
        "​\n",
        "​\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "def feature_selection_customized_last(id_column, train_data, variance_threshold=0.005, max_missing_percentage=0.9, correlation_threshold=0.85):\n",
        "    \"\"\"\n",
        "    Perform feature selection based on variance, missing values, and correlation.\n",
        "\n",
        "    Args:\n",
        "    - id_column (string): ID of the row\n",
        "    - train_data (pd.DataFrame): Training data.\n",
        "    - variance_threshold (float): Threshold for variance-based feature selection.\n",
        "    - max_missing_percentage (float): Maximum allowed percentage of missing values.\n",
        "    - correlation_threshold (float): Threshold for correlation-based feature selection.\n",
        "\n",
        "    Returns:\n",
        "    - selected_features (set): Set of selected features.\n",
        "    \"\"\"\n",
        "    # 0. Replacing inf values\n",
        "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # 1. Variance Threshold\n",
        "    if variance_threshold > 0:\n",
        "        numerical_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        numerical_cols = [col for col in numerical_cols if col != id_column]\n",
        "        selector = VarianceThreshold(threshold=variance_threshold)\n",
        "        selector.fit(train_data[numerical_cols])\n",
        "        cols_to_keep_vt = set(train_data[numerical_cols].columns[selector.get_support(indices=True)].tolist())\n",
        "        print(f\"Variables after Variance Threshold: {len(cols_to_keep_vt)}\")\n",
        "    else:\n",
        "        cols_to_keep_vt = set(train_data.columns)\n",
        "\n",
        "    # Filtra las columnas basadas en la varianza\n",
        "    train_data_filtered_vt = train_data[list(cols_to_keep_vt)]\n",
        "\n",
        "    # 2. Missing Values\n",
        "    if max_missing_percentage > 0:\n",
        "        # Calcula el porcentaje de valores faltantes para cada columna\n",
        "        missing_percentage = train_data_filtered_vt.isnull().mean()\n",
        "        # Identifica las columnas que tienen un porcentaje de valores faltantes mayor que el máximo permitido\n",
        "        cols_to_drop_na = missing_percentage[missing_percentage > max_missing_percentage].index\n",
        "        # Mantén las columnas que tienen un porcentaje de valores faltantes menor o igual al máximo permitido\n",
        "        cols_to_keep_na = set(train_data_filtered_vt.columns) - set(cols_to_drop_na)\n",
        "        print(f\"Variables after Missing Values Threshold: {len(cols_to_keep_na)}\")\n",
        "    else:\n",
        "        cols_to_keep_na = set(train_data_filtered_vt.columns)\n",
        "\n",
        "    # Filtra las columnas basadas en valores faltantes\n",
        "    train_data_filtered_na = train_data_filtered_vt[list(cols_to_keep_na)]\n",
        "\n",
        "    # 3. Correlation Matrix Elimination\n",
        "    if correlation_threshold > 0:\n",
        "        correlated_features = set()\n",
        "        correlation_matrix = train_data_filtered_na.corr().abs()\n",
        "        upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "        for column in upper_triangle.columns:\n",
        "            if any(upper_triangle[column] > correlation_threshold):\n",
        "                correlated_features.add(column)\n",
        "        cols_to_keep_corr = set(train_data_filtered_na.columns) - set(correlated_features)\n",
        "        print(f\"Variables after Correlation Threshold: {len(cols_to_keep_corr)}\")\n",
        "    else:\n",
        "        cols_to_keep_corr = set(train_data_filtered_na.columns)\n",
        "\n",
        "    # Devuelve la intersección de las características seleccionadas por varianza, valores faltantes y correlación\n",
        "    selected_features = cols_to_keep_corr.union(set([id_column]))\n",
        "    print(f\"Total selected features: {len(selected_features)}\")\n",
        "\n",
        "    return selected_features\n",
        "​\n",
        "def downcast_df_int_columns(dfname, df):\n",
        "    list_of_columns = list(df.select_dtypes(include=[\"int32\", \"int64\"]).columns)\n",
        "​\n",
        "    if len(list_of_columns)>=1:\n",
        "        max_string_length = max([len(col) for col in list_of_columns]) # finds max string length for better status printing\n",
        "        print(\"downcasting integers for:\", list_of_columns, \"\\n\")\n",
        "​\n",
        "        for col in list_of_columns:\n",
        "            print(\"reduced memory usage for:  \", col.ljust(max_string_length+2)[:max_string_length+2],\n",
        "                  \"from\", str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8), \"to\", end=\" \")\n",
        "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
        "            print(str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8))\n",
        "    else:\n",
        "        print(\"no columns to downcast\")\n",
        "​\n",
        "    gc.collect()\n",
        "​\n",
        "    print(dfname, \" - 1. Done\")\n",
        "​\n",
        "def downcast_df_float_columns(dfname, df):\n",
        "    list_of_columns = list(df.select_dtypes(include=[\"float64\"]).columns)\n",
        "​\n",
        "    if len(list_of_columns)>=1:\n",
        "        max_string_length = max([len(col) for col in list_of_columns]) # finds max string length for better status printing\n",
        "        print(\"downcasting float for:\", list_of_columns, \"\\n\")\n",
        "​\n",
        "        for col in list_of_columns:\n",
        "            print(\"reduced memory usage for:  \", col.ljust(max_string_length+2)[:max_string_length+2],\n",
        "                  \"from\", str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8), \"to\", end=\" \")\n",
        "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
        "            print(str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8))\n",
        "    else:\n",
        "        print(\"no columns to downcast\")\n",
        "​\n",
        "    gc.collect()\n",
        "​\n",
        "    print(dfname, \" - 2. Done\")\n",
        "​\n",
        "def convert_columns_to_catg(dfname, df, column_list):\n",
        "    for col in column_list:\n",
        "        print(\"converting\", col.ljust(30), \"size: \", round(df[col].memory_usage(deep=True)*1e-6,2), end=\"\\t\")\n",
        "        df[col] = df[col].astype(\"category\")\n",
        "        print(\"->\\t\", round(df[col].memory_usage(deep=True)*1e-6,2))\n",
        "    print(dfname, \" - 3. Done\")\n",
        "​\n",
        "def compress_dset (dfname, df):\n",
        "    downcast_df_int_columns(dfname, df)\n",
        "    downcast_df_float_columns(dfname, df)\n",
        "\n",
        "\n",
        "## Get size of objects\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "​\n",
        "# List objects and their sizes\n",
        "def list_objects_size():\n",
        "    global_vars = globals().items()\n",
        "    var_sizes = {var: sys.getsizeof(value) for var, value in global_vars}\n",
        "    sorted_vars = sorted(var_sizes.items(), key=lambda x: x[1], reverse=True)\n",
        "    for var, size in sorted_vars[0:100]:\n",
        "        print(f\"{var}: {size} bytes\")"
      ],
      "metadata": {
        "id": "Md_nViH5aBVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "ybU49pxQamVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: Agrupación de datos de OpportunityPostVenta por OpportunityID"
      ],
      "metadata": {
        "id": "4umCkLveaob0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectura de datos\n",
        "OpportunityItemPostVenta_prepro = pd.read_csv('gs://ferreyros-datathon-bucket-01/OpportunityItemPostVenta.csv')"
      ],
      "metadata": {
        "id": "fT2LExdraxfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversión\n",
        "OpportunityItemPostVenta_prepro['OpportunityID'] = OpportunityItemPostVenta_prepro['OpportunityID'].astype('object')\n",
        "\n",
        "# Registros Duplicados\n",
        "\n",
        "print(f\"Registros antes de eliminar duplicados: {OpportunityItemPostVenta_prepro.shape[0]}\")\n",
        "OpportunityItemPostVenta_prepro.drop_duplicates(inplace=True)\n",
        "print(f\"Registros después de eliminar duplicados: {OpportunityItemPostVenta_prepro.shape[0]}\")"
      ],
      "metadata": {
        "id": "bS43h-dSa06a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fx_porc_missings(OpportunityItemPostVenta_prepro, only_missings=True)"
      ],
      "metadata": {
        "id": "YRNn1o91a08c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de variables flags (_missing)\n",
        "create_columns_based_nulls(OpportunityItemPostVenta_prepro,exclude_cols=None,name='field')\n",
        "\n",
        "# Remover valores nulos\n",
        "OpportunityItemPostVenta_prepro['ProductModel'].fillna('DESCONOCIDO', inplace=True)\n",
        "OpportunityItemPostVenta_prepro['ProductBrand'].fillna('DESCONOCIDO', inplace=True)\n",
        "OpportunityItemPostVenta_prepro['ProductProductCategory'].fillna('DESCONOCIDO', inplace=True)\n",
        "OpportunityItemPostVenta_prepro['ProductLine'].fillna('DESCONOCIDO', inplace=True)\n",
        "OpportunityItemPostVenta_prepro['ProductMarket'].fillna('DESCONOCIDO', inplace=True)\n",
        "\n",
        "OpportunityItemPostVenta_prepro['QuantityProduct'].fillna(OpportunityItemPostVenta_prepro['QuantityProduct'].median(), inplace=True)\n",
        "OpportunityItemPostVenta_prepro['ValueNegotiatedUSD'].fillna(OpportunityItemPostVenta_prepro['ValueNegotiatedUSD'].median(), inplace=True)"
      ],
      "metadata": {
        "id": "a5Y0yRaMa0-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminación de variables:\n",
        "\n",
        "- **ValueNegotiatedCurrency**. No agregará información para el modelo ya que se trabaja únicamente con el tipo de moneda: 'USD'.\n",
        "- **ValueNegotiated**. Se decide trabajar con montos en dólares.\n",
        "- **DocumentClassificationDescription**. Se debe eliminar ya que es una variable de varianza cero. Todos sus valores son iguales. Columna constante."
      ],
      "metadata": {
        "id": "gx3YQnNKa6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OpportunityItemPostVenta_prepro.drop(columns=['ValueNegotiatedCurrency', 'ValueNegotiated'], inplace=True)\n",
        "OpportunityItemPostVenta_prepro.drop(columns=['DocumentClassificationDescription'], inplace=True)\n",
        "\n",
        "fx_porc_missings(OpportunityItemPostVenta_prepro, only_missings=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxbiFaYMa1C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ProductID** no se imputa ya que son identificadores únicos."
      ],
      "metadata": {
        "id": "7IBkiF9RbA4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a agrupar las categorías de cada variable según su frecuencia de participación. Hubiese sido valioso hacerlo por el Target sin embargo no contamos con esa información en esta tabla"
      ],
      "metadata": {
        "id": "962qj6vcbA6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar las variables\n",
        "\n",
        "# Definir los conjuntos de códigos\n",
        "\n",
        "codigo_agd_dict = {\n",
        "    'ProductModel': ['DESCONOCIDO','Modelo3','Modelo17','Modelo85','Modelo53','Modelo83','Modelo101','Modelo15','Modelo55','Modelo54'],\n",
        "    'ProductProductCategory': ['DESCONOCIDO','Categoria22','Categoria18','Categoria7','Categoria9','Categoria8','Categoria21','Categoria14','Categoria4','Categoria5'],\n",
        "    'ProductLine': ['DESCONOCIDO','Linea2','Linea3','Linea1','Linea19','Linea5','Linea18'],\n",
        "    'ProductMarket': ['DESCONOCIDO','Mercado7','Mercado2','Mercado10','Mercado9','Mercado8']\n",
        "}\n",
        "\n",
        "# Función para aplicar las reglas\n",
        "def codigo_agd_funcion(x, col_name):\n",
        "    if x in codigo_agd_dict[col_name]:\n",
        "        return x\n",
        "    else:\n",
        "        if col_name == 'ProductModel':\n",
        "            return 'OTROS_MODELOS'\n",
        "        elif col_name == 'ProductProductCategory':\n",
        "            return 'OTRAS_CATEGORIAS'\n",
        "        elif col_name == 'ProductLine':\n",
        "            return 'OTRAS_LINEAS'\n",
        "        elif col_name == 'ProductMarket':\n",
        "            return 'OTROS_MERCADOS'\n",
        "\n",
        "# Aplicar las reglas a cada columna\n",
        "OpportunityItemPostVenta_prepro['ProductModel'] = OpportunityItemPostVenta_prepro['ProductModel'].apply(lambda x: codigo_agd_funcion(x, 'ProductModel'))\n",
        "OpportunityItemPostVenta_prepro['ProductProductCategory'] = OpportunityItemPostVenta_prepro['ProductProductCategory'].apply(lambda x: codigo_agd_funcion(x, 'ProductProductCategory'))\n",
        "OpportunityItemPostVenta_prepro['ProductLine'] = OpportunityItemPostVenta_prepro['ProductLine'].apply(lambda x: codigo_agd_funcion(x, 'ProductLine'))\n",
        "OpportunityItemPostVenta_prepro['ProductMarket'] = OpportunityItemPostVenta_prepro['ProductMarket'].apply(lambda x: codigo_agd_funcion(x, 'ProductMarket'))\n",
        "\n",
        "# Reemplazo para ProductBrand\n",
        "OpportunityItemPostVenta_prepro['ProductBrand'] = OpportunityItemPostVenta_prepro['ProductBrand'].replace(['Marca2', 'Marca3'], 'Marca1')"
      ],
      "metadata": {
        "id": "HrqZc8Xta1FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de la variable **ValuePerUnit** y de otras tomando en cuenta esta nueva variable creada"
      ],
      "metadata": {
        "id": "4Mj5TbqabIX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "OpportunityItemPostVenta_prepro['ValueNegotiatedUSD_is_zero'] = (OpportunityItemPostVenta_prepro['ValueNegotiatedUSD'] == 0).astype(int)\n",
        "\n",
        "min_value_neg_usd = OpportunityItemPostVenta_prepro[OpportunityItemPostVenta_prepro['ValueNegotiatedUSD'] > 0]['ValueNegotiatedUSD'].min()\n",
        "OpportunityItemPostVenta_prepro['ValueNegotiatedUSD'].replace(0, min_value_neg_usd, inplace=True)\n",
        "\n",
        "min_value_qp = OpportunityItemPostVenta_prepro[OpportunityItemPostVenta_prepro['QuantityProduct'] > 0]['QuantityProduct'].min()\n",
        "OpportunityItemPostVenta_prepro['QuantityProduct'].replace(0, min_value_qp, inplace=True)\n",
        "\n",
        "OpportunityItemPostVenta_prepro['ValuePerUnit']=OpportunityItemPostVenta_prepro['ValueNegotiatedUSD']/OpportunityItemPostVenta_prepro['QuantityProduct']\n",
        "#OpportunityItemPostVenta_prepro['ValuePerUnit'].fillna(0, inplace=True)\n",
        "\n",
        "OpportunityItemPostVenta_prepro['PropQuantityProduct'] = OpportunityItemPostVenta_prepro['QuantityProduct'] / OpportunityItemPostVenta_prepro.groupby('OpportunityID')['QuantityProduct'].transform('sum')\n",
        "OpportunityItemPostVenta_prepro['PropValueNegotiatedUSD'] = OpportunityItemPostVenta_prepro['ValueNegotiatedUSD'] / OpportunityItemPostVenta_prepro.groupby('OpportunityID')['ValueNegotiatedUSD'].transform('sum')\n",
        "\n",
        "OpportunityItemPostVenta_prepro['LogQuantityProduct'] = np.log1p(OpportunityItemPostVenta_prepro['QuantityProduct'])\n",
        "OpportunityItemPostVenta_prepro['LogValueNegotiatedUSD'] = np.log1p(OpportunityItemPostVenta_prepro['ValueNegotiatedUSD'])\n",
        "OpportunityItemPostVenta_prepro['LogValuePerUnit'] = np.log1p(OpportunityItemPostVenta_prepro['ValuePerUnit'])\n",
        "\n",
        "OpportunityItemPostVenta_prepro['CumSumQuantityProduct'] = OpportunityItemPostVenta_prepro.groupby('OpportunityID')['QuantityProduct'].cumsum()\n",
        "OpportunityItemPostVenta_prepro['CumSumValueNegotiatedUSD'] = OpportunityItemPostVenta_prepro.groupby('OpportunityID')['ValueNegotiatedUSD'].cumsum()\n",
        "#OpportunityItemPostVenta_prepro['CumMeanValueNegotiatedUSD'] = OpportunityItemPostVenta_prepro.groupby('OpportunityID')['ValueNegotiatedUSD'].cummean()\n",
        "#OpportunityItemPostVenta_prepro['CumMeanQuantityProduct'] = OpportunityItemPostVenta_prepro.groupby('OpportunityID')['QuantityProduct'].cummean()\n",
        "\n",
        "OpportunityItemPostVenta_prepro['RelValuePerUnit_ProductModel'] = OpportunityItemPostVenta_prepro['ValuePerUnit'] / OpportunityItemPostVenta_prepro.groupby('ProductModel')['ValuePerUnit'].transform('mean')\n",
        "OpportunityItemPostVenta_prepro['RelValuePerUnit_ProductBrand'] = OpportunityItemPostVenta_prepro['ValuePerUnit'] / OpportunityItemPostVenta_prepro.groupby('ProductBrand')['ValuePerUnit'].transform('mean')\n",
        "OpportunityItemPostVenta_prepro['RelValuePerUnit_ProductProductCategory'] = OpportunityItemPostVenta_prepro['ValuePerUnit'] / OpportunityItemPostVenta_prepro.groupby('ProductProductCategory')['ValuePerUnit'].transform('mean')\n",
        "OpportunityItemPostVenta_prepro['RelValuePerUnit_ProductLine'] = OpportunityItemPostVenta_prepro['ValuePerUnit'] / OpportunityItemPostVenta_prepro.groupby('ProductLine')['ValuePerUnit'].transform('mean')\n",
        "OpportunityItemPostVenta_prepro['RelValuePerUnit_ProductMarket'] = OpportunityItemPostVenta_prepro['ValuePerUnit'] / OpportunityItemPostVenta_prepro.groupby('ProductMarket')['ValuePerUnit'].transform('mean')"
      ],
      "metadata": {
        "id": "ffCTCqzia1HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fx_porc_missings(OpportunityItemPostVenta_prepro, only_missings=True)"
      ],
      "metadata": {
        "id": "vcUhftEOa1Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se generar agrupaciones tomando en cuenta las variables cualitativas y variables cuantitativas. Se generan agregaciones entre cada una de las categorías de las variables cualitativas y las variables cuantitativas"
      ],
      "metadata": {
        "id": "EqO2sUXggzIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "quantitative_vars = ['QuantityProduct', 'ValueNegotiatedUSD']\n",
        "\n",
        "flag_vars = ['ProductID_missing_field', 'ProductModel_missing_field', 'ProductBrand_missing_field', 'ProductProductCategory_missing_field',\n",
        "             'ProductLine_missing_field', 'ProductMarket_missing_field', 'QuantityProduct_missing_field', 'ValueNegotiated_missing_field',\n",
        "             'ValueNegotiatedCurrency_missing_field', 'ValueNegotiatedUSD_missing_field']\n",
        "\n",
        "new_quantitative_vars = ['ValuePerUnit', 'PropQuantityProduct', 'PropValueNegotiatedUSD', 'LogQuantityProduct',\n",
        "                         'LogValueNegotiatedUSD', 'LogValuePerUnit', 'CumSumQuantityProduct', 'CumSumValueNegotiatedUSD',\n",
        "                         'RelValuePerUnit_ProductModel', 'RelValuePerUnit_ProductBrand', 'RelValuePerUnit_ProductProductCategory',\n",
        "                         'RelValuePerUnit_ProductLine', 'RelValuePerUnit_ProductMarket','ValueNegotiatedUSD_is_zero']\n",
        "\n",
        "quantitative_vars = quantitative_vars + new_quantitative_vars #+ flag_vars\n",
        "\n",
        "# Lista de variables cualitativas\n",
        "qualitative_vars = ['ProductModel', 'ProductBrand', 'ProductProductCategory', 'ProductLine', 'ProductMarket']\n",
        "\n",
        "# Diccionario para almacenar los resultados de las agregaciones\n",
        "agg_results = {}\n",
        "\n",
        "# Agregación general de 'count'\n",
        "general_count_agg = OpportunityItemPostVenta_prepro.groupby('OpportunityID').size().reset_index()\n",
        "general_count_agg.columns = ['OpportunityID', 'count_general_opportunity_id']\n",
        "\n",
        "# Agregación de 'count' para las variables cualitativas\n",
        "for qual_var in qualitative_vars:\n",
        "    qual_vars_count_agg = OpportunityItemPostVenta_prepro.groupby(['OpportunityID', qual_var])[qual_var].agg(['count']).unstack().reset_index()\n",
        "    qual_vars_count_agg.columns = ['OpportunityID'] + [f'{col[0]}_{col[1]}_{qual_var}' for col in qual_vars_count_agg.columns[1:]]\n",
        "    agg_results[f'{qual_var}_count'] = qual_vars_count_agg\n",
        "\n",
        "# Realizar las agregaciones para cada variable cuantitativa\n",
        "for quant_var in quantitative_vars:\n",
        "    # Agrupaciones generales\n",
        "    general_others_agg = OpportunityItemPostVenta_prepro.groupby('OpportunityID')[quant_var].agg(['sum', 'mean', 'median', 'std', 'min', 'max']).reset_index()\n",
        "    prefixes = ['sum', 'mean', 'median', 'std', 'min', 'max']\n",
        "    general_others_agg.columns = ['OpportunityID'] + [f'{prefix}_general_{quant_var}' for prefix in prefixes]\n",
        "    agg_results[f'general_{quant_var}'] = general_others_agg\n",
        "\n",
        "    # Realizar las agregaciones para cada variable cualitativa\n",
        "    for qual_var in qualitative_vars:\n",
        "        agg = OpportunityItemPostVenta_prepro.groupby(['OpportunityID', qual_var])[quant_var].agg(['sum', 'mean', 'median', 'std', 'min', 'max']).unstack().reset_index()\n",
        "        agg.columns = ['OpportunityID'] + [f'{col[0]}_{col[1]}_{qual_var}_{quant_var}' for col in agg.columns[1:]]\n",
        "        agg_results[f'{qual_var}_{quant_var}'] = agg\n",
        "\n",
        "# Combinar todas las tablas en un solo DataFrame\n",
        "OpportunityItemPostVenta_prepro_agg = pd.DataFrame({'OpportunityID': OpportunityItemPostVenta_prepro['OpportunityID'].unique()})\n",
        "\n",
        "# Añadir la agregación de 'count' general al DataFrame combinado\n",
        "OpportunityItemPostVenta_prepro_agg = OpportunityItemPostVenta_prepro_agg.merge(general_count_agg, on='OpportunityID', how='left')\n",
        "\n",
        "# Añadir las otras agregaciones al DataFrame combinado\n",
        "for key, df in agg_results.items():\n",
        "    OpportunityItemPostVenta_prepro_agg = OpportunityItemPostVenta_prepro_agg.merge(df, on='OpportunityID', how='left')\n",
        "\n",
        "# Manejar NaNs en las tablas agregadas\n",
        "#OpportunityItemPostVenta_prepro_agg = OpportunityItemPostVenta_prepro_agg.fillna(9999)"
      ],
      "metadata": {
        "id": "fogOZGmpg2OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(OpportunityItemPostVenta_prepro_agg.shape)\n",
        "print(OpportunityItemPostVenta_prepro_agg.dtypes.unique())\n",
        "print(OpportunityItemPostVenta_prepro_agg.dtypes[OpportunityItemPostVenta_prepro_agg.dtypes == 'O'])"
      ],
      "metadata": {
        "id": "g9uKlPxSg2Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compress_dset(\"OpportunityItemPostVenta_prepro_agg\", OpportunityItemPostVenta_prepro_agg)"
      ],
      "metadata": {
        "id": "Rq7zjukvg2S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list_objects_size()\n",
        "del agg,df,qual_vars_count_agg,general_others_agg,general_count_agg,agg_results,OpportunityItemPostVenta_prepro"
      ],
      "metadata": {
        "id": "XdHkQQiCg2VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Obtener Insights de X_train_df y X_test_df"
      ],
      "metadata": {
        "id": "UQCzSvsAg88o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos\n",
        "X_train_df_prepro = pd.read_csv('gs://ferreyros-datathon-bucket-01/X_train_df.csv')\n",
        "X_test_df_prepro = pd.read_csv('gs://ferreyros-datathon-bucket-01/X_test_df.csv')"
      ],
      "metadata": {
        "id": "i3aW_g3ng2Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_df_prepro.shape)\n",
        "print(X_test_df_prepro.shape)"
      ],
      "metadata": {
        "id": "L6OkuBARhEOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversión de datos\n",
        "X_train_df_prepro['OpportunityID'] = X_train_df_prepro['OpportunityID'].astype('object')\n",
        "X_train_df_prepro['OpportunityStartDate'] = pd.to_datetime(X_train_df_prepro['OpportunityStartDate'])\n",
        "\n",
        "X_test_df_prepro['OpportunityID'] = X_test_df_prepro['OpportunityID'].astype('object')\n",
        "X_test_df_prepro['OpportunityStartDate'] = pd.to_datetime(X_test_df_prepro['OpportunityStartDate'])"
      ],
      "metadata": {
        "id": "L5o1uKrMhEQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remover valores nulos\n",
        "variables_moda = ['CustomerClassDescription', 'CustomerTypeDescription','CustomerMainMarketDescription',\n",
        "                  'CustomerEconomicActivityDescription','CustomerAddressDepartmentDescription']\n",
        "\n",
        "for col in variables_moda:\n",
        "    moda = X_train_df_prepro[col].mode()[0]\n",
        "    X_train_df_prepro[col].fillna(moda, inplace=True)\n",
        "\n",
        "\n",
        "create_columns_based_nulls(X_train_df_prepro,exclude_cols=['CustomerID'],name='field')\n",
        "X_train_df_prepro['CustomerTaxpayerTypeDescription'].fillna('DESCONOCIDO', inplace=True)\n",
        "\n",
        "\n",
        "for col in variables_moda:\n",
        "    moda = X_train_df_prepro[col].mode()[0]\n",
        "    X_test_df_prepro[col].fillna(moda, inplace=True)\n",
        "\n",
        "create_columns_based_nulls(X_test_df_prepro,exclude_cols=['CustomerID'],name='field')\n",
        "X_test_df_prepro['CustomerTaxpayerTypeDescription'].fillna('DESCONOCIDO', inplace=True)"
      ],
      "metadata": {
        "id": "TV848OvThEW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar las categorías tomando en cuenta la conversión con el target\n",
        "\n",
        "# Diccionario de configuraciones\n",
        "config = {\n",
        "    'LeadSourceDescription': {\n",
        "        'values': ['Fuente10', 'Fuente1', 'Fuente9', 'Fuente8'],\n",
        "        'label': 'OTRAS_FUENTES'\n",
        "    },\n",
        "    'OpportunitySalesRepGroupDescription': {\n",
        "        'replace': {'Cuenta1': 'Cuenta5'}\n",
        "    },\n",
        "    'OpportunitySalesOfficeDescription': {\n",
        "        'values': ['Oficina5', 'Oficina11', 'Oficina9', 'Oficina2', 'Oficina14', 'Oficina12', 'Oficina3', 'Oficina1', 'Oficina8', 'Oficina7', 'Oficina17', 'Oficina4', 'Oficina6'],\n",
        "        'label': 'OTRAS_OFICINAS'\n",
        "    },\n",
        "    'CustomerTypeDescription': {\n",
        "        'replace': {'Tipo10': 'Tipo17', 'Tipo11': 'Tipo2'},\n",
        "        'values': ['Tipo2', 'Tipo16', 'Tipo4', 'Tipo15', 'Tipo14', 'Tipo17', 'Tipo3', 'Tipo9'],\n",
        "        'label': 'OTROS_TIPOS'\n",
        "    },\n",
        "    'CustomerMainMarketDescription': {\n",
        "        'replace': {'MercadoCliente7': 'MercadoCliente5'},\n",
        "        'values': ['MercadoCliente9', 'MercadoCliente2', 'MercadoCliente10', 'MercadoCliente14', 'MercadoCliente4', 'MercadoCliente12', 'MercadoCliente11', 'MercadoCliente5', 'MercadoCliente3', 'MercadoCliente15'],\n",
        "        'label': 'OTROS_MERCADOS'\n",
        "    },\n",
        "    'CustomerVerticalIndustryDescription': {\n",
        "        'replace': {'Industria4': 'Industria1'}\n",
        "    },\n",
        "    'CustomerTaxpayerTypeDescription': {\n",
        "        'replace': {'TipoEmpresa21': 'TipoEmpresa17', 'TipoEmpresa22': 'TipoEmpresa17', 'TipoEmpresa9': 'TipoEmpresa1', 'TipoEmpresa23': 'TipoEmpresa1'},\n",
        "        'values': ['TipoEmpresa17', 'TipoEmpresa2', 'TipoEmpresa1', 'TipoEmpresa19', 'DESCONOCIDO', 'TipoEmpresa14', 'TipoEmpresa16', 'TipoEmpresa7', 'TipoEmpresa18'],\n",
        "        'label': 'OTROS_TIPOS_EMPRESAS'\n",
        "    },\n",
        "    'CustomerAddressDepartmentDescription': {\n",
        "        'replace': {'Departamento10': 'Departamento1', 'Departamento22': 'Departamento14'},\n",
        "        'values': ['Departamento1', 'Departamento14', 'Departamento6', 'Departamento5', 'Departamento7', 'Departamento11', 'Departamento20', 'Departamento15', 'Departamento2', 'Departamento4', 'Departamento3', 'Departamento12', 'Departamento18', 'Departamento17', 'Departamento13', 'Departamento8'],\n",
        "        'label': 'OTROS_DEPARTAMENTOS'\n",
        "    },\n",
        "    'CustomerEconomicActivityDescription': {\n",
        "        'replacements': {\n",
        "            'top_1_Activity': ['Actividad95', 'Actividad13', 'Actividad6', 'Actividad100', 'Actividad82', 'Actividad93', 'Actividad106'],\n",
        "            'top_2_Activity': ['Actividad12', 'Actividad7', 'Actividad46', 'Actividad8', 'Actividad52', 'Actividad53', 'Actividad49', 'Actividad54', 'Actividad4', 'Actividad101', 'Actividad44'],\n",
        "            'top_3_Activity': ['Actividad17', 'Actividad91', 'Actividad40', 'Actividad51', 'Actividad104', 'Actividad68', 'Actividad118', 'Actividad105', 'Actividad94', 'Actividad42', 'Actividad16', 'Actividad89', 'Actividad90', 'Actividad102', 'Actividad63', 'Actividad80', 'Actividad38', 'Actividad98', 'Actividad73', 'Actividad87', 'Actividad31'],\n",
        "            'top_4_Activity': ['Actividad92', 'Actividad86', 'Actividad50', 'Actividad71', 'Actividad74', 'Actividad79', 'Actividad36', 'Actividad45', 'Actividad103', 'Actividad23', 'Actividad83', 'Actividad109', 'Actividad1', 'Actividad56', 'Actividad24', 'Actividad66', 'Actividad64', 'Actividad110', 'Actividad34', 'Actividad30', 'Actividad81', 'Actividad37', 'Actividad28', 'Actividad116'],\n",
        "            'top_5_Activity': ['Actividad59', 'Actividad35', 'Actividad29', 'Actividad120', 'Actividad70', 'Actividad113', 'Actividad27', 'Actividad2', 'Actividad25', 'Actividad72', 'Actividad22', 'Actividad39', 'Actividad84', 'Actividad11', 'Actividad9', 'Actividad76', 'Actividad55'],\n",
        "            'top_6_Activity': ['Actividad85', 'Actividad69', 'Actividad88', 'Actividad96', 'Actividad62', 'Actividad115', 'Actividad112', 'Actividad60', 'Actividad15'],\n",
        "            'top_7_Activity': ['Actividad48', 'Actividad47', 'Actividad61', 'Actividad14', 'Actividad121', 'Actividad41', 'Actividad97', 'Actividad77', 'Actividad43', 'Actividad75', 'Actividad65', 'Actividad99']\n",
        "        },\n",
        "        'values': ['top_1_Activity', 'top_2_Activity', 'top_3_Activity', 'top_4_Activity', 'top_5_Activity', 'top_6_Activity', 'top_7_Activity'],\n",
        "        'label': 'top_8_Activity'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Función para aplicar las reglas\n",
        "def apply_rules(df, config):\n",
        "    for col, rules in config.items():\n",
        "        if 'replace' in rules:\n",
        "            df[col] = df[col].replace(rules['replace'])\n",
        "        if 'replacements' in rules:\n",
        "            for new_value, old_values in rules['replacements'].items():\n",
        "                df[col] = df[col].replace(old_values, new_value)\n",
        "        if 'values' in rules:\n",
        "            values = rules['values']\n",
        "            label = rules['label']\n",
        "            df[col] = df[col].apply(lambda x: x if x in values else label)\n",
        "    return df\n",
        "\n",
        "# Aplicar las reglas a los DataFrames\n",
        "X_train_df_prepro = apply_rules(X_train_df_prepro, config)\n",
        "X_test_df_prepro = apply_rules(X_test_df_prepro, config)"
      ],
      "metadata": {
        "id": "3F3roFM3hEY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "\n",
        "# Función para crear características basadas en la fecha de oportunidad\n",
        "def create_date_features(df):\n",
        "    df['Year'] = df['OpportunityStartDate'].dt.year\n",
        "    df['Quarter'] = df['OpportunityStartDate'].dt.quarter\n",
        "    df['DayOfWeek'] = df['OpportunityStartDate'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['OpportunityStartDate'].dt.dayofyear\n",
        "    df['Period'] = df['OpportunityStartDate'].dt.strftime('%Y%m')\n",
        "    df['Month'] = df['OpportunityStartDate'].dt.month\n",
        "    df['WeekOfYear'] = df['OpportunityStartDate'].dt.isocalendar().week\n",
        "    df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
        "    df['IsMonthEnd'] = df['OpportunityStartDate'].dt.is_month_end.astype(int)\n",
        "    df['IsQuarterEnd'] = df['OpportunityStartDate'].dt.is_quarter_end.astype(int)\n",
        "    df['IsYearEnd'] = df['OpportunityStartDate'].dt.is_year_end.astype(int)\n",
        "\n",
        "    today = pd.Timestamp.today()\n",
        "    df['DaysSinceToday'] = (today - df['OpportunityStartDate']).dt.days\n",
        "\n",
        "    start_of_year = pd.to_datetime(df['OpportunityStartDate'].dt.year.astype(str) + '-01-01')\n",
        "    df['DaysSinceStartOfYear'] = (df['OpportunityStartDate'] - start_of_year).dt.days\n",
        "\n",
        "    df['DaysSinceStartOfQuarter'] = (df['OpportunityStartDate'] - df['OpportunityStartDate'].dt.to_period(\"Q\").dt.start_time).dt.days\n",
        "\n",
        "    start_of_month = pd.to_datetime(df['OpportunityStartDate'].dt.year.astype(str) + '-' + df['OpportunityStartDate'].dt.month.astype(str) + '-01')\n",
        "    df['DaysSinceStartOfMonth'] = (df['OpportunityStartDate'] - start_of_month).dt.days\n",
        "\n",
        "    return df\n",
        "\n",
        "# Función para crear características basadas en los IDs de clientes y equipos\n",
        "def create_id_features(df):\n",
        "    customer_opportunity_count = df.groupby('CustomerID').size().reset_index(name='CustomerOpportunityCount')\n",
        "    df = df.merge(customer_opportunity_count, on='CustomerID', how='left')\n",
        "\n",
        "    equipment_opportunity_count = df.groupby('EquipmentSerialNumber').size().reset_index(name='EquipmentOpportunityCount')\n",
        "    df = df.merge(equipment_opportunity_count, on='EquipmentSerialNumber', how='left')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Función para crear características de agregaciones temporales\n",
        "def create_temporal_aggregations(df):\n",
        "    last_month = df['OpportunityStartDate'].max() - DateOffset(months=1)\n",
        "    last_quarter = df['OpportunityStartDate'].max() - DateOffset(months=3)\n",
        "    last_year = df['OpportunityStartDate'].max() - DateOffset(years=1)\n",
        "\n",
        "    df['OpportunitiesLastMonth'] = df['OpportunityStartDate'].apply(lambda x: sum((df['OpportunityStartDate'] >= last_month) & (df['OpportunityStartDate'] <= x)))\n",
        "    df['OpportunitiesLastQuarter'] = df['OpportunityStartDate'].apply(lambda x: sum((df['OpportunityStartDate'] >= last_quarter) & (df['OpportunityStartDate'] <= x)))\n",
        "    df['OpportunitiesLastYear'] = df['OpportunityStartDate'].apply(lambda x: sum((df['OpportunityStartDate'] >= last_year) & (df['OpportunityStartDate'] <= x)))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Función para crear interacciones entre variables\n",
        "def create_interaction_features(df):\n",
        "    df['DocClass_CustClass'] = df['OpportunityDocumentClassificationDescription'] + '_' + df['CustomerClassDescription']\n",
        "    df['DocClass_LeadSource'] = df['OpportunityDocumentClassificationDescription'] + '_' + df['LeadSourceDescription']\n",
        "    df['DocClass_SalesRepGroup'] = df['OpportunityDocumentClassificationDescription'] + '_' + df['OpportunitySalesRepGroupDescription']\n",
        "    #df['DocClass_SalesOffice'] = df['OpportunityDocumentClassificationDescription'] + '_' + df['OpportunitySalesOfficeDescription']\n",
        "\n",
        "    df['CustClass_Type'] = df['CustomerClassDescription'] + '_' + df['CustomerTypeDescription']\n",
        "    df['CustClass_Market'] = df['CustomerClassDescription'] + '_' + df['CustomerMainMarketDescription']\n",
        "    df['CustClass_EconActivity'] = df['CustomerClassDescription'] + '_' + df['CustomerEconomicActivityDescription']\n",
        "    df['CustClass_Industry'] = df['CustomerClassDescription'] + '_' + df['CustomerVerticalIndustryDescription']\n",
        "\n",
        "    df['LeadSource_SalesRepGroup'] = df['LeadSourceDescription'] + '_' + df['OpportunitySalesRepGroupDescription']\n",
        "    df['LeadSource_SalesOffice'] = df['LeadSourceDescription'] + '_' + df['OpportunitySalesOfficeDescription'] #ok\n",
        "    #df['LeadSource_Type'] = df['LeadSourceDescription'] + '_' + df['CustomerTypeDescription']\n",
        "    #df['LeadSource_Market'] = df['LeadSourceDescription'] + '_' + df['CustomerMainMarketDescription']\n",
        "\n",
        "    df['SalesRepGroup_SalesOffice'] = df['OpportunitySalesRepGroupDescription'] + '_' + df['OpportunitySalesOfficeDescription'] #ok\n",
        "    df['SalesRepGroup_Type'] = df['OpportunitySalesRepGroupDescription'] + '_' + df['CustomerTypeDescription'] #ok\n",
        "    df['SalesRepGroup_Market'] = df['OpportunitySalesRepGroupDescription'] + '_' + df['CustomerMainMarketDescription'] #ok\n",
        "    df['SalesRepGroup_EconActivity'] = df['OpportunitySalesRepGroupDescription'] + '_' + df['CustomerEconomicActivityDescription']\n",
        "\n",
        "    #df['SalesOffice_Type'] = df['OpportunitySalesOfficeDescription'] + '_' + df['CustomerTypeDescription']\n",
        "    #df['SalesOffice_Market'] = df['OpportunitySalesOfficeDescription'] + '_' + df['CustomerMainMarketDescription']\n",
        "    #df['SalesOffice_EconActivity'] = df['OpportunitySalesOfficeDescription'] + '_' + df['CustomerEconomicActivityDescription']\n",
        "\n",
        "    #df['Type_Market'] = df['CustomerTypeDescription'] + '_' + df['CustomerMainMarketDescription']\n",
        "    #df['Type_EconActivity'] = df['CustomerTypeDescription'] + '_' + df['CustomerEconomicActivityDescription']\n",
        "    df['Type_Industry'] = df['CustomerTypeDescription'] + '_' + df['CustomerVerticalIndustryDescription']\n",
        "    #df['Type_Taxpayer'] = df['CustomerTypeDescription'] + '_' + df['CustomerTaxpayerTypeDescription']\n",
        "\n",
        "    #df['Market_EconActivity'] = df['CustomerMainMarketDescription'] + '_' + df['CustomerEconomicActivityDescription']\n",
        "    df['Market_Industry'] = df['CustomerMainMarketDescription'] + '_' + df['CustomerVerticalIndustryDescription']\n",
        "    #df['Market_Taxpayer'] = df['CustomerMainMarketDescription'] + '_' + df['CustomerTaxpayerTypeDescription']\n",
        "    #df['Market_Department'] = df['CustomerMainMarketDescription'] + '_' + df['CustomerAddressDepartmentDescription']\n",
        "\n",
        "    df['EconActivity_Industry'] = df['CustomerEconomicActivityDescription'] + '_' + df['CustomerVerticalIndustryDescription']\n",
        "    #df['EconActivity_Taxpayer'] = df['CustomerEconomicActivityDescription'] + '_' + df['CustomerTaxpayerTypeDescription']\n",
        "    #df['EconActivity_Department'] = df['CustomerEconomicActivityDescription'] + '_' + df['CustomerAddressDepartmentDescription']\n",
        "\n",
        "    df['Industry_Taxpayer'] = df['CustomerVerticalIndustryDescription'] + '_' + df['CustomerTaxpayerTypeDescription']\n",
        "    #df['Industry_Department'] = df['CustomerVerticalIndustryDescription'] + '_' + df['CustomerAddressDepartmentDescription']\n",
        "\n",
        "    #df['Taxpayer_Department'] = df['CustomerTaxpayerTypeDescription'] + '_' + df['CustomerAddressDepartmentDescription']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Función para crear características de tendencia de tiempo\n",
        "def create_trend_features(df):\n",
        "    first_opportunity = df.groupby('CustomerID')['OpportunityStartDate'].min().reset_index(name='FirstOpportunityDate')\n",
        "    last_opportunity = df.groupby('CustomerID')['OpportunityStartDate'].max().reset_index(name='LastOpportunityDate')\n",
        "    df = df.merge(first_opportunity, on='CustomerID', how='left')\n",
        "    df = df.merge(last_opportunity, on='CustomerID', how='left')\n",
        "\n",
        "    df['DaysSinceFirstOpportunity'] = (df['OpportunityStartDate'] - df['FirstOpportunityDate']).dt.days\n",
        "    df['DaysSinceLastOpportunity'] = (df['OpportunityStartDate'] - df['LastOpportunityDate']).dt.days\n",
        "\n",
        "    df.drop(columns=['FirstOpportunityDate', 'LastOpportunityDate'], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Función para aplicar todas las transformaciones\n",
        "def apply_all_transformations(df):\n",
        "    df = create_date_features(df)\n",
        "    df = create_id_features(df)\n",
        "    df = create_temporal_aggregations(df)\n",
        "    df = create_interaction_features(df)\n",
        "    df = create_trend_features(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Aplicar todas las transformaciones a X_train_df_prepro\n",
        "X_train_df_prepro = apply_all_transformations(X_train_df_prepro)\n",
        "\n",
        "# Aplicar todas las transformaciones a X_test_df_prepro\n",
        "X_test_df_prepro = apply_all_transformations(X_test_df_prepro)\n",
        "\n",
        "#Print\n",
        "print(X_train_df_prepro.shape)\n",
        "print(X_test_df_prepro.shape)"
      ],
      "metadata": {
        "id": "YAa8kQOMhEbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reemplazar los valores nulos finales. El resto será imputado con 9999\n",
        "\n",
        "median_coc = X_train_df_prepro['CustomerOpportunityCount'].median()\n",
        "median_dsfo = X_train_df_prepro['DaysSinceFirstOpportunity'].median()\n",
        "median_dslo = X_train_df_prepro['DaysSinceLastOpportunity'].median()\n",
        "\n",
        "X_train_df_prepro['CustomerOpportunityCount'].fillna(median_coc, inplace=True)\n",
        "X_train_df_prepro['DaysSinceFirstOpportunity'].fillna(median_dsfo, inplace=True)\n",
        "X_train_df_prepro['DaysSinceLastOpportunity'].fillna(median_dslo, inplace=True)\n",
        "\n",
        "X_test_df_prepro['CustomerOpportunityCount'].fillna(median_coc, inplace=True)\n",
        "X_test_df_prepro['DaysSinceFirstOpportunity'].fillna(median_dsfo, inplace=True)\n",
        "X_test_df_prepro['DaysSinceLastOpportunity'].fillna(median_dslo, inplace=True)"
      ],
      "metadata": {
        "id": "ZxVQJbvOhEdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fx_porc_missings(X_train_df_prepro, only_missings=True)\n",
        "fx_porc_missings(X_test_df_prepro, only_missings=True)"
      ],
      "metadata": {
        "id": "2mYhQdEwhEfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar los datos antes de ejecutar un modelo\n",
        "\n",
        "def preprocess_data(df, is_train=True):\n",
        "    # Eliminar variables innecesarias\n",
        "    df = df.drop(columns=['OpportunityStartDate'])\n",
        "\n",
        "    if is_train:\n",
        "        # Guardar y luego eliminar la variable objetivo\n",
        "        target = df['ClosedWonOpportunity']\n",
        "        df = df.drop(columns=['ClosedWonOpportunity'])\n",
        "\n",
        "    # Variables booleanas\n",
        "    df['VentaWeb'] = df['VentaWeb'].astype(int)\n",
        "\n",
        "    # Variables categóricas\n",
        "    categorical_vars = [\n",
        "        'OpportunityDocumentClassificationDescription', 'CustomerClassDescription', 'LeadSourceDescription',\n",
        "        'OpportunitySalesRepGroupDescription', 'OpportunitySalesOfficeDescription', 'CustomerTypeDescription',\n",
        "        'CustomerMainMarketDescription', 'CustomerEconomicActivityDescription', 'CustomerVerticalIndustryDescription',\n",
        "        'CustomerTaxpayerTypeDescription', 'CustomerAddressDepartmentDescription', 'Period',\n",
        "        'DocClass_CustClass','DocClass_LeadSource','DocClass_SalesRepGroup',\n",
        "        'CustClass_Type','CustClass_Market','CustClass_EconActivity','CustClass_Industry',\n",
        "        'LeadSource_SalesRepGroup','LeadSource_SalesOffice',\n",
        "        'SalesRepGroup_SalesOffice','SalesRepGroup_Type','SalesRepGroup_Market','SalesRepGroup_EconActivity',\n",
        "        'Type_Industry',\n",
        "        'Market_Industry',\n",
        "        'EconActivity_Industry',\n",
        "        'Industry_Taxpayer']\n",
        "\n",
        "    # Crear variables dummy asegurándose de que los resultados sean binarios (0 y 1)\n",
        "    df = pd.get_dummies(df, columns=categorical_vars, drop_first=True, dtype=int)\n",
        "\n",
        "    if is_train:\n",
        "        return df, target\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "# Preprocesar los datos de entrenamiento y prueba\n",
        "X_train_df_prepro, y_train = preprocess_data(X_train_df_prepro, is_train=True)\n",
        "X_test_df_prepro = preprocess_data(X_test_df_prepro, is_train=False)\n",
        "\n",
        "# Verificar si las columnas de entrenamiento y prueba son iguales\n",
        "missing_cols_in_test = set(X_train_df_prepro.columns) - set(X_test_df_prepro.columns)\n",
        "for col in missing_cols_in_test:\n",
        "    X_test_df_prepro[col] = 0\n",
        "\n",
        "missing_cols_in_train = set(X_test_df_prepro.columns) - set(X_train_df_prepro.columns)\n",
        "for col in missing_cols_in_train:\n",
        "    X_train_df_prepro[col] = 0\n",
        "\n",
        "# Asegurar que el orden de las columnas sea el mismo\n",
        "X_test_df_prepro = X_test_df_prepro[X_train_df_prepro.columns]\n",
        "\n",
        "# Añadir la variable objetivo al final del conjunto de entrenamiento\n",
        "X_train_df_prepro['ClosedWonOpportunity'] = y_train"
      ],
      "metadata": {
        "id": "RiM0Cqo-hEhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar los tipos de datos\n",
        "print(X_train_df_prepro.shape)\n",
        "print(X_train_df_prepro.dtypes.unique())\n",
        "print(X_train_df_prepro.dtypes[X_train_df_prepro.dtypes == 'O'])\n",
        "\n",
        "print(\"---------------\")\n",
        "\n",
        "# Validar los tipos de datos\n",
        "print(X_test_df_prepro.shape)\n",
        "print(X_test_df_prepro.dtypes.unique())\n",
        "print(X_test_df_prepro.dtypes[X_test_df_prepro.dtypes == 'O'])"
      ],
      "metadata": {
        "id": "ScY7w30shkys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compress_dset(\"X_train_df_prepro\", X_train_df_prepro)\n",
        "#compress_dset(\"X_test_df_prepro\", X_test_df_prepro)"
      ],
      "metadata": {
        "id": "2xnQ4Nn3hk0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list_objects_size()\n",
        "del y_train,missing_cols_in_test,missing_cols_in_train"
      ],
      "metadata": {
        "id": "Rrcxm5zqhk20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Agrupación de datos en EquipmentPostVenta por CurrentCustomerID"
      ],
      "metadata": {
        "id": "EmPHmlWmhpn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectura de datos\n",
        "EquipmentPostVenta_prepro = pd.read_csv('gs://ferreyros-datathon-bucket-01/EquipmentPostVenta.csv')"
      ],
      "metadata": {
        "id": "qq3Fc2Ethk45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectura de valores nulos\n",
        "fx_porc_missings(EquipmentPostVenta_prepro, only_missings=True)"
      ],
      "metadata": {
        "id": "yGNq1X5ghk7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de variables flags (_missing)\n",
        "create_columns_based_nulls(EquipmentPostVenta_prepro,exclude_cols=None,name='field')\n",
        "\n",
        "# Imputación de valores nulos\n",
        "EquipmentPostVenta_prepro['BrandDescription'].fillna('DESCONOCIDO', inplace=True)\n",
        "EquipmentPostVenta_prepro['FamilyDescription'].fillna('DESCONOCIDO', inplace=True)\n",
        "EquipmentPostVenta_prepro['LineDescription'].fillna('DESCONOCIDO', inplace=True)\n",
        "EquipmentPostVenta_prepro['MarketDescription'].fillna('DESCONOCIDO', inplace=True)\n",
        "EquipmentPostVenta_prepro['ApplicationCodeDescription'].fillna('DESCONOCIDO', inplace=True)\n",
        "EquipmentPostVenta_prepro['Manufacturer'].fillna('DESCONOCIDO', inplace=True)\n",
        "EquipmentPostVenta_prepro['ItemType'].fillna('DESCONOCIDO', inplace=True)\n",
        "EquipmentPostVenta_prepro['FamilyDescriptionLLM'].fillna('DESCONOCIDO', inplace=True)"
      ],
      "metadata": {
        "id": "5tQ34t1ehk9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputación en YearOfProduction\n",
        "median_year = EquipmentPostVenta_prepro['YearOfProduction'].median()\n",
        "EquipmentPostVenta_prepro['YearOfProduction'].fillna(median_year, inplace=True)\n",
        "\n",
        "inconsistent_values = [0.0, 201.0, 202.0]\n",
        "\n",
        "valid_data = EquipmentPostVenta_prepro[~EquipmentPostVenta_prepro['YearOfProduction'].isin(inconsistent_values)]\n",
        "median_year_of_production = valid_data['YearOfProduction'].median()\n",
        "EquipmentPostVenta_prepro['YearOfProduction'] = EquipmentPostVenta_prepro['YearOfProduction'].replace(inconsistent_values, median_year_of_production)\n",
        "\n",
        "# Creación de la variable Antiguedad\n",
        "current_year = datetime.now().year\n",
        "EquipmentPostVenta_prepro['Antiguedad'] = current_year - EquipmentPostVenta_prepro['YearOfProduction']\n",
        "\n",
        "print(\"EquipmentPostVenta_prepro\")\n",
        "EquipmentPostVenta_prepro.shape"
      ],
      "metadata": {
        "id": "Vej_XnmVhk_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar las categorías según su frecuencia relativa\n",
        "# Diccionario de configuraciones\n",
        "config = {\n",
        "    'EquipmentTypeDescription': {\n",
        "        'values': ['EquipoTipo1', 'EquipoTipo8', 'EquipoTipo7'],\n",
        "        'label': 'OTROS_EQUIPOS_TIPO'\n",
        "    },\n",
        "    'BrandDescription': {\n",
        "        'values': ['Marca9', 'DESCONOCIDO', 'Marca36'],\n",
        "        'label': 'OTRAS_MARCAS'\n",
        "    },\n",
        "    'FamilyDescription': {\n",
        "        'values': ['Familia18', 'Familia51', 'Familia84', 'Familia87', 'Familia19', 'Familia99', 'Familia86', 'Familia43','Familia89', 'Familia71', 'Familia23', 'Familia31', 'Familia96', 'Familia20', 'Familia54', 'Familia21',\n",
        "                  'Familia91', 'Familia59', 'Familia48', 'Familia41', 'Familia29', 'Familia56', 'Familia95', 'Familia35','DESCONOCIDO'],\n",
        "        'label': 'OTRAS_FAMILIAS'\n",
        "    },\n",
        "    'LineDescription': {\n",
        "        'values': ['DESCONOCIDO', 'Linea6', 'Linea3', 'Linea1', 'Linea5', 'Linea8'],\n",
        "        'label': 'OTRAS_LINEAS'\n",
        "    },\n",
        "    'MarketDescription': {\n",
        "        'values': ['DESCONOCIDO', 'Mercado5', 'Mercado2', 'Mercado9', 'Mercado8', 'Mercado13', 'Mercado12'],\n",
        "        'label': 'OTROS_MERCADOS'\n",
        "    },\n",
        "    'ApplicationCodeDescription': {\n",
        "        'values': ['DESCONOCIDO', 'TRABAJO MEDIO', 'TRABAJO LIGERO', 'TRABAJO PESADO', 'MOTOR PRINCIPAL', 'GENERADOR STANDBY', 'PRIME'],\n",
        "        'label': 'OTROS_TIPOS_DE_TRABAJO'\n",
        "    },\n",
        "    'Manufacturer': {\n",
        "        'values': ['Fabricante14', 'Fabricante46'],\n",
        "        'label': 'OTROS_FABRICANTES'\n",
        "    },\n",
        "    'FamilyDescriptionLLM': {\n",
        "        'values': ['FamiliaDescripcion25', 'FamiliaDescripcion30', 'FamiliaDescripcion4', 'FamiliaDescripcion6', 'FamiliaDescripcion11', 'FamiliaDescripcion7', 'FamiliaDescripcion10',\n",
        "                  'FamiliaDescripcion9','FamiliaDescripcion15','FamiliaDescripcion24','FamiliaDescripcion22','FamiliaDescripcion14','FamiliaDescripcion26','DESCONOCIDO'],\n",
        "        'label': 'OTRAS_FAMILIA_DESCRIPCION'\n",
        "    },\n",
        "}\n",
        "\n",
        "# Función para aplicar las reglas\n",
        "def apply_rules(df, config):\n",
        "    for col, rules in config.items():\n",
        "        if 'values' in rules:\n",
        "            values = rules['values']\n",
        "            label = rules['label']\n",
        "            df[col] = df[col].apply(lambda x: x if x in values else label)\n",
        "    return df\n",
        "\n",
        "# Aplicar las reglas a los DataFrames\n",
        "EquipmentPostVenta_prepro = apply_rules(EquipmentPostVenta_prepro, config)\n",
        "\n",
        "# Reemplazar\n",
        "EquipmentPostVenta_prepro['ApplicationCodeDescription'] = EquipmentPostVenta_prepro['ApplicationCodeDescription'].replace('TRABAJO MEDIO', 'TRABAJO_MEDIO')\n",
        "EquipmentPostVenta_prepro['ApplicationCodeDescription'] = EquipmentPostVenta_prepro['ApplicationCodeDescription'].replace('TRABAJO LIGERO', 'TRABAJO_LIGERO')\n",
        "EquipmentPostVenta_prepro['ApplicationCodeDescription'] = EquipmentPostVenta_prepro['ApplicationCodeDescription'].replace('TRABAJO PESADO', 'TRABAJO_PESADO')\n",
        "EquipmentPostVenta_prepro['ApplicationCodeDescription'] = EquipmentPostVenta_prepro['ApplicationCodeDescription'].replace('MOTOR PRINCIPAL', 'MOTOR_PRINCIPAL')\n",
        "EquipmentPostVenta_prepro['ApplicationCodeDescription'] = EquipmentPostVenta_prepro['ApplicationCodeDescription'].replace('GENERADOR STANDBY', 'GENERADOR_STANDBY')"
      ],
      "metadata": {
        "id": "eRpkWN13hy-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "quantitative_vars = ['YearOfProduction', 'Antiguedad']\n",
        "\n",
        "flag_vars = ['BrandDescription_missing_field', 'FamilyDescription_missing_field', 'LineDescription_missing_field', 'MarketDescription_missing_field',\n",
        "             'YearOfProduction_missing_field', 'ApplicationCodeDescription_missing_field', 'Manufacturer_missing_field', 'ItemType_missing_field',\n",
        "             'FamilyDescriptionLLM_missing_field']\n",
        "\n",
        "#quantitative_vars = quantitative_vars + flag_vars\n",
        "\n",
        "# Lista de variables cualitativas\n",
        "qualitative_vars = ['EquipmentTypeDescription', 'BrandDescription', 'FamilyDescription','LineDescription', 'MarketDescription', 'ApplicationCodeDescription','Manufacturer', 'ItemType', 'FamilyDescriptionLLM']\n",
        "\n",
        "# Diccionario para almacenar los resultados de las agregaciones\n",
        "agg_results = {}\n",
        "\n",
        "# Agregación general de 'count'\n",
        "general_count_agg = EquipmentPostVenta_prepro.groupby('CurrentCustomerID').size().reset_index()\n",
        "general_count_agg.columns = ['CurrentCustomerID', 'count_general_customer_id']\n",
        "\n",
        "# Agregación de 'count' para las variables cualitativas\n",
        "for qual_var in qualitative_vars:\n",
        "    qual_vars_count_agg = EquipmentPostVenta_prepro.groupby(['CurrentCustomerID', qual_var])[qual_var].agg(['count']).unstack().reset_index()\n",
        "    qual_vars_count_agg.columns = ['CurrentCustomerID'] + [f'{col[0]}_{col[1]}_{qual_var}' for col in qual_vars_count_agg.columns[1:]]\n",
        "    agg_results[f'{qual_var}_count'] = qual_vars_count_agg\n",
        "\n",
        "# Realizar las agregaciones para cada variable cuantitativa\n",
        "for quant_var in quantitative_vars:\n",
        "    # Agrupaciones generales\n",
        "    general_others_agg = EquipmentPostVenta_prepro.groupby('CurrentCustomerID')[quant_var].agg(['sum', 'mean', 'median', 'std', 'min', 'max']).reset_index()\n",
        "    prefixes = ['sum', 'mean', 'median', 'std', 'min', 'max']\n",
        "    general_others_agg.columns = ['CurrentCustomerID'] + [f'{prefix}_general_{quant_var}' for prefix in prefixes]\n",
        "    agg_results[f'general_{quant_var}'] = general_others_agg\n",
        "\n",
        "    # Realizar las agregaciones para cada variable cualitativa\n",
        "    for qual_var in qualitative_vars:\n",
        "        agg = EquipmentPostVenta_prepro.groupby(['CurrentCustomerID', qual_var])[quant_var].agg(['sum', 'mean', 'median', 'std', 'min', 'max']).unstack().reset_index()\n",
        "        agg.columns = ['CurrentCustomerID'] + [f'{col[0]}_{col[1]}_{qual_var}_{quant_var}' for col in agg.columns[1:]]\n",
        "        agg_results[f'{qual_var}_{quant_var}'] = agg\n",
        "\n",
        "# Combinar todas las tablas en un solo DataFrame\n",
        "EquipmentPostVenta_prepro_agg = pd.DataFrame({'CurrentCustomerID': EquipmentPostVenta_prepro['CurrentCustomerID'].unique()})\n",
        "\n",
        "# Añadir la agregación de 'count' general al DataFrame combinado\n",
        "EquipmentPostVenta_prepro_agg = EquipmentPostVenta_prepro_agg.merge(general_count_agg, on='CurrentCustomerID', how='left')\n",
        "\n",
        "# Añadir las otras agregaciones al DataFrame combinado\n",
        "for key, df in agg_results.items():\n",
        "    EquipmentPostVenta_prepro_agg = EquipmentPostVenta_prepro_agg.merge(df, on='CurrentCustomerID', how='left')\n",
        "\n",
        "# Manejar NaNs en las tablas agregadas\n",
        "#OpportunityItemPostVenta_prepro_agg = OpportunityItemPostVenta_prepro_agg.fillna(9999)"
      ],
      "metadata": {
        "id": "tT9SOaIghzA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar los tipos de datos\n",
        "print(EquipmentPostVenta_prepro_agg.shape)\n",
        "print(EquipmentPostVenta_prepro_agg.dtypes.unique())\n",
        "print(EquipmentPostVenta_prepro_agg.dtypes[EquipmentPostVenta_prepro_agg.dtypes == 'O'])"
      ],
      "metadata": {
        "id": "rVnRMgY_hzC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compress_dset(\"EquipmentPostVenta_prepro_agg\", EquipmentPostVenta_prepro_agg)"
      ],
      "metadata": {
        "id": "S_gr9JvehzE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir a dummy las variables de EquipmentPostVenta_prepro\n",
        "\n",
        "# Variables categóricas\n",
        "categorical_vars = ['EquipmentTypeDescription', 'BrandDescription', 'FamilyDescription','LineDescription', 'MarketDescription',\n",
        "                    'ApplicationCodeDescription','Manufacturer', 'ItemType', 'FamilyDescriptionLLM']\n",
        "\n",
        "# Crear variables dummy asegurándose de que los resultados sean binarios (0 y 1)\n",
        "EquipmentPostVenta_prepro = pd.get_dummies(EquipmentPostVenta_prepro, columns=categorical_vars, drop_first=True, dtype=int)"
      ],
      "metadata": {
        "id": "R45mISNmhzHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar los tipos de datos\n",
        "print(EquipmentPostVenta_prepro.shape)\n",
        "print(EquipmentPostVenta_prepro.dtypes.unique())\n",
        "print(EquipmentPostVenta_prepro.dtypes[EquipmentPostVenta_prepro.dtypes == 'O'])"
      ],
      "metadata": {
        "id": "N7XFPO-UhlB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compress_dset(\"EquipmentPostVenta_prepro\", EquipmentPostVenta_prepro)"
      ],
      "metadata": {
        "id": "PU9jLTUPiI-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list_objects_size()\n",
        "del valid_data,agg,df,qual_vars_count_agg,general_others_agg,general_count_agg,agg_results"
      ],
      "metadata": {
        "id": "1ukSD87siKz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4: Agrupación de datos de horometros_vl por SerialNumber y generación de nuevas variables"
      ],
      "metadata": {
        "id": "v1cfD4P6iMj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectura de datos\n",
        "horometros_vl_prepro = pd.read_csv('gs://ferreyros-datathon-bucket-01/horometros_vl.csv')"
      ],
      "metadata": {
        "id": "-4E_o-F6iK2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horometros_vl_prepro.info()"
      ],
      "metadata": {
        "id": "lPF0s9WOiU00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar la cantidad de repetidos por Month\n",
        "print(horometros_vl_prepro.shape[0])\n",
        "print(horometros_vl_prepro.SerialNumber.nunique())"
      ],
      "metadata": {
        "id": "IU6rgEdIiU5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar\n",
        "(horometros_vl_prepro[\"SerialNumber\"] + horometros_vl_prepro['Month'].astype(str)).nunique()"
      ],
      "metadata": {
        "id": "LL9SzqvOiU7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto demuestra que no se tiene repetidos de **Month** para cada **SerialNumber**"
      ],
      "metadata": {
        "id": "IQSvs1kQiZOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "# Extraer características del Month\n",
        "horometros_vl_prepro['Month'] = pd.to_datetime(horometros_vl_prepro['Month'])\n",
        "\n",
        "# Extraer features de Month\n",
        "horometros_vl_prepro['Year'] = horometros_vl_prepro['Month'].dt.year\n",
        "horometros_vl_prepro['MonthF'] = horometros_vl_prepro['Month'].dt.month\n",
        "horometros_vl_prepro['MonthM'] = 'M' + horometros_vl_prepro['MonthF'].astype(str)\n",
        "horometros_vl_prepro['Quarter'] = horometros_vl_prepro['Month'].dt.quarter\n",
        "horometros_vl_prepro['QuarterQ'] = 'Q' + horometros_vl_prepro['Quarter'].astype(str)\n",
        "horometros_vl_prepro['YearQuarter'] = horometros_vl_prepro['Month'].dt.to_period('Q').astype(str)\n",
        "#horometros_vl_prepro['DayOfWeek'] = horometros_vl_prepro['Month'].dt.dayofweek #0:Monday\n",
        "horometros_vl_prepro['NameDayOfWeek'] = horometros_vl_prepro['Month'].dt.strftime('%A')\n",
        "#horometros_vl_prepro['DayOfYear'] = horometros_vl_prepro['Month'].dt.dayofyear\n",
        "#horometros_vl_prepro['WeekOfYear'] = horometros_vl_prepro['Month'].dt.isocalendar().week\n",
        "horometros_vl_prepro['Period'] = horometros_vl_prepro['Month'].dt.strftime('%Y%m')\n",
        "# Día del mes\n",
        "#horometros_vl_prepro['DayOfMonth'] = horometros_vl_prepro['Month'].dt.day\n",
        "# Semana del mes\n",
        "#horometros_vl_prepro['WeekOfMonth'] = horometros_vl_prepro['Month'].apply(lambda x: (x.day-1) // 7 + 1)\n",
        "\n",
        "# Días desde la primera fecha registrada hasta cada registro (teniendo en cuenta SerialNumber)\n",
        "horometros_vl_prepro['DaysSinceFirstDate'] = horometros_vl_prepro.groupby('SerialNumber')['Month'].transform(lambda x: (x - x.min()).dt.days)\n",
        "\n",
        "# Días hasta el final del mes para cada registro\n",
        "horometros_vl_prepro['DaysUntilEndOfMonth'] = horometros_vl_prepro['Month'].dt.days_in_month - horometros_vl_prepro['Month'].dt.day\n",
        "\n",
        "# Si la fecha está cerca del inicio o final del mes (primera semana/última semana del mes)\n",
        "#horometros_vl_prepro['IsBeginningOfMonth'] = (horometros_vl_prepro['Month'].dt.day <= 7).astype(int)\n",
        "#horometros_vl_prepro['IsEndOfMonth'] = (horometros_vl_prepro['Month'].dt.day >= horometros_vl_prepro['Month'].dt.days_in_month - 7).astype(int)"
      ],
      "metadata": {
        "id": "4oSAhOugiU90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contar el número de **SerialNumber** presentes en 1,2,3,4,5,6 años."
      ],
      "metadata": {
        "id": "LgqHIOj_ifmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "unique_years = horometros_vl_prepro['Year'].unique()\n",
        "print(f'Años únicos: {unique_years}')\n",
        "\n",
        "pivot_table = horometros_vl_prepro.pivot_table(index='SerialNumber', columns='Year', aggfunc='size', fill_value=0)\n",
        "\n",
        "serials_counts = (pivot_table > 0).sum(axis=1)\n",
        "counts_by_years = serials_counts.value_counts().sort_index()\n",
        "\n",
        "for years, count in counts_by_years.items():\n",
        "    print(f'Número de SerialNumber presentes en {years} años: {count}')"
      ],
      "metadata": {
        "id": "FDr2pLK2iVCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cantidad de items por **SerialNumber**"
      ],
      "metadata": {
        "id": "-QYAr2saikD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horometros_vl_prepro.groupby('SerialNumber')['SerialNumber'].count()"
      ],
      "metadata": {
        "id": "8QsVlm3piVEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Max\n",
        "horometros_vl_prepro.groupby('SerialNumber')['Month'].nunique().max()"
      ],
      "metadata": {
        "id": "Uw8A62d2il21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horometros_vl_prepro.loc[horometros_vl_prepro.SerialNumber=='fefd2b07607dbc0f',:]"
      ],
      "metadata": {
        "id": "CXUQw1KTinmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "quantitative_vars = ['HorasTrabajadas1Mes', 'Horometro','HorasTrabajadas3Meses','HorasTrabajadas6Meses','HorasTrabajadas12Meses','DaysSinceFirstDate','DaysUntilEndOfMonth']\n",
        "\n",
        "# Lista de variables cualitativas\n",
        "qualitative_vars = ['Year', 'MonthM', 'QuarterQ', 'YearQuarter','NameDayOfWeek']\n",
        "\n",
        "# Diccionario para almacenar los resultados de las agregaciones\n",
        "agg_results = {}\n",
        "\n",
        "# Agregación general de 'count'\n",
        "general_count_agg = horometros_vl_prepro.groupby('SerialNumber').size().reset_index()\n",
        "general_count_agg.columns = ['SerialNumber', 'count_serial_number_id']\n",
        "\n",
        "# Agregación de 'count' para las variables cualitativas\n",
        "for qual_var in qualitative_vars:\n",
        "    qual_vars_count_agg = horometros_vl_prepro.groupby(['SerialNumber', qual_var])[qual_var].agg(['count']).unstack().reset_index()\n",
        "    qual_vars_count_agg.columns = ['SerialNumber'] + [f'{col[0]}_{col[1]}_{qual_var}' for col in qual_vars_count_agg.columns[1:]]\n",
        "    agg_results[f'{qual_var}_count'] = qual_vars_count_agg\n",
        "\n",
        "# Realizar las agregaciones para cada variable cuantitativa\n",
        "for quant_var in quantitative_vars:\n",
        "    # Agrupaciones generales\n",
        "    general_others_agg = horometros_vl_prepro.groupby('SerialNumber')[quant_var].agg(['sum', 'mean', 'median', 'std', 'min', 'max']).reset_index()\n",
        "    prefixes = ['sum', 'mean', 'median', 'std', 'min', 'max']\n",
        "    general_others_agg.columns = ['SerialNumber'] + [f'{prefix}_general_{quant_var}' for prefix in prefixes]\n",
        "    agg_results[f'general_{quant_var}'] = general_others_agg\n",
        "\n",
        "    # Realizar las agregaciones para cada variable cualitativa\n",
        "    for qual_var in qualitative_vars:\n",
        "        agg = horometros_vl_prepro.groupby(['SerialNumber', qual_var])[quant_var].agg(['sum', 'mean', 'median', 'std', 'min', 'max']).unstack().reset_index()\n",
        "        agg.columns = ['SerialNumber'] + [f'{col[0]}_{col[1]}_{qual_var}_{quant_var}' for col in agg.columns[1:]]\n",
        "        agg_results[f'{qual_var}_{quant_var}'] = agg\n",
        "\n",
        "# Combinar todas las tablas en un solo DataFrame\n",
        "horometros_vl_prepro_agg = pd.DataFrame({'SerialNumber': horometros_vl_prepro['SerialNumber'].unique()})\n",
        "\n",
        "# Añadir la agregación de 'count' general al DataFrame combinado\n",
        "horometros_vl_prepro_agg = horometros_vl_prepro_agg.merge(general_count_agg, on='SerialNumber', how='left')\n",
        "\n",
        "# Añadir las otras agregaciones al DataFrame combinado\n",
        "for key, df in agg_results.items():\n",
        "    horometros_vl_prepro_agg = horometros_vl_prepro_agg.merge(df, on='SerialNumber', how='left')\n",
        "\n",
        "# Agregar más estadísticas si es necesario\n",
        "# Ejemplo: Coeficiente de variación (std / mean)\n",
        "horometros_vl_prepro_agg['CoefVarHorasTrabajadas'] = horometros_vl_prepro_agg['std_general_HorasTrabajadas1Mes'] / horometros_vl_prepro_agg['mean_general_HorasTrabajadas1Mes']\n",
        "horometros_vl_prepro_agg['CoefVarHorometro'] = horometros_vl_prepro_agg['std_general_Horometro'] / horometros_vl_prepro_agg['mean_general_Horometro']\n",
        "\n",
        "# Manejar NaNs en las tablas agregadas\n",
        "#OpportunityItemPostVenta_prepro_agg = OpportunityItemPostVenta_prepro_agg.fillna(9999)\n",
        "\n",
        "#PRINT\n",
        "horometros_vl_prepro_agg.shape"
      ],
      "metadata": {
        "id": "YGbVM3FtinpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "# Calcular la diferencia entre la primera y la última fecha y la suma de HorasTrabajadas1Mes\n",
        "horometros_agg = horometros_vl_prepro.groupby('SerialNumber').agg(\n",
        "    FirstDate=('Month', 'min'),\n",
        "    LastDate=('Month', 'max'),\n",
        "    FirstHorometro=('Horometro', 'first'),\n",
        "    LastHorometro=('Horometro', 'last'),\n",
        "    FirstHorasTrabajadas1Mes=('HorasTrabajadas1Mes', 'first'),\n",
        "    LastHorasTrabajadas1Mes=('HorasTrabajadas1Mes', 'last'),\n",
        "    FirstHorasTrabajadas3Mes=('HorasTrabajadas3Meses', 'first'),\n",
        "    LastHorasTrabajadas3Mes=('HorasTrabajadas3Meses', 'last'),\n",
        "    FirstHorasTrabajadas6Mes=('HorasTrabajadas6Meses', 'first'),\n",
        "    LastHorasTrabajadas6Mes=('HorasTrabajadas6Meses', 'last'),\n",
        "    FirstHorasTrabajadas12Mes=('HorasTrabajadas12Meses', 'first'),\n",
        "    LastHorasTrabajadas12Mes=('HorasTrabajadas12Meses', 'last')\n",
        ").reset_index()\n",
        "\n",
        "# Calcular la duración en días y la diferencia en Horometro\n",
        "horometros_agg['DurationDays'] = (horometros_agg['LastDate'] - horometros_agg['FirstDate']).dt.days\n",
        "horometros_agg['DiffHorometro'] = horometros_agg['LastHorometro'] - horometros_agg['FirstHorometro']\n",
        "horometros_agg['DiffLastHorasTrabajadas1Mes'] = horometros_agg['LastHorasTrabajadas1Mes'] - horometros_agg['FirstHorasTrabajadas1Mes']\n",
        "horometros_agg['DiffLastHorasTrabajadas3Mes'] = horometros_agg['LastHorasTrabajadas3Mes'] - horometros_agg['FirstHorasTrabajadas3Mes']\n",
        "horometros_agg['DiffLastHorasTrabajadas6Mes'] = horometros_agg['LastHorasTrabajadas6Mes'] - horometros_agg['FirstHorasTrabajadas6Mes']\n",
        "horometros_agg['DiffLastHorasTrabajadas12Mes'] = horometros_agg['LastHorasTrabajadas12Mes'] - horometros_agg['FirstHorasTrabajadas12Mes']\n",
        "\n",
        "# Eliminar columnas innecesarias\n",
        "horometros_agg.drop(columns=['FirstDate', 'LastDate', 'FirstHorometro', 'LastHorometro',\n",
        "                             'FirstHorasTrabajadas1Mes','LastHorasTrabajadas1Mes',\n",
        "                             'FirstHorasTrabajadas3Mes','LastHorasTrabajadas3Mes',\n",
        "                             'LastHorasTrabajadas6Mes','FirstHorasTrabajadas6Mes',\n",
        "                             'FirstHorasTrabajadas12Mes','LastHorasTrabajadas12Mes'], inplace=True)\n",
        "\n",
        "horometros_agg.shape"
      ],
      "metadata": {
        "id": "pZBQ4tlLinrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cruzar los Datasets\n",
        "horometros_vl_prepro_agg = horometros_vl_prepro_agg.merge(horometros_agg,\n",
        "                                            left_on=['SerialNumber'],\n",
        "                                            right_on=['SerialNumber'],\n",
        "                                            how='left')"
      ],
      "metadata": {
        "id": "sl9OaWJ3itkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar los tipos de datos\n",
        "print(horometros_vl_prepro_agg.shape)\n",
        "print(horometros_vl_prepro_agg.dtypes.unique())\n",
        "print(horometros_vl_prepro_agg.dtypes[horometros_vl_prepro_agg.dtypes == 'O'])"
      ],
      "metadata": {
        "id": "vnK02PQuitmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compress_dset(\"EquipmentPostVenta_prepro\", EquipmentPostVenta_prepro)"
      ],
      "metadata": {
        "id": "-Ej2hFSyito0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list_objects_size()\n",
        "del horometros_vl_prepro,agg,df,qual_vars_count_agg,pivot_table,general_others_agg,general_count_agg,serials_counts,counts_by_years,agg_results,horometros_agg"
      ],
      "metadata": {
        "id": "Sz9u37NOitrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 5: Cruzar X_train_df y X_test_df"
      ],
      "metadata": {
        "id": "f2YWF99Oi0cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(OpportunityItemPostVenta_prepro_agg.shape)\n",
        "print(X_train_df_prepro.shape)\n",
        "print(X_test_df_prepro.shape)\n",
        "print(EquipmentPostVenta_prepro.shape)\n",
        "print(EquipmentPostVenta_prepro_agg.shape)\n",
        "print(horometros_vl_prepro_agg.shape)"
      ],
      "metadata": {
        "id": "T4UUV56YittZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cruzar todos los Datasets\n",
        "\n",
        "# Realizar el primer cruce en X_train_df_prepro\n",
        "X_train_df_prepro = X_train_df_prepro.merge(OpportunityItemPostVenta_prepro_agg,\n",
        "                                            left_on=['OpportunityID'],\n",
        "                                            right_on=['OpportunityID'],\n",
        "                                            how='left')\n",
        "\n",
        "# Realizar el segundo cruce en X_train_df_prepro\n",
        "X_train_df_prepro = X_train_df_prepro.merge(EquipmentPostVenta_prepro,\n",
        "                                            left_on=['CustomerID', 'EquipmentSerialNumber'],\n",
        "                                            right_on=['CurrentCustomerID', 'EquipmentID'],\n",
        "                                            how='left')\n",
        "\n",
        "# Realizar el tercer cruce en X_train_df_prepro\n",
        "X_train_df_prepro = X_train_df_prepro.merge(EquipmentPostVenta_prepro_agg,\n",
        "                                            left_on=['CustomerID'],\n",
        "                                            right_on=['CurrentCustomerID'],\n",
        "                                            how='left')\n",
        "\n",
        "\n",
        "# Realizar el cuarto cruce en X_train_df_prepro\n",
        "X_train_df_prepro = X_train_df_prepro.merge(horometros_vl_prepro_agg,\n",
        "                                            left_on=['EquipmentSerialNumber'],\n",
        "                                            right_on=['SerialNumber'],\n",
        "                                            how='left')\n",
        "\n",
        "# Eliminar las llaves redundantes de X_train_df_prepro\n",
        "X_train_df_prepro.drop(columns=['CurrentCustomerID_x', 'EquipmentID', 'CurrentCustomerID_y','SerialNumber','EquipmentSerialNumber','CustomerID'], inplace=True)\n",
        "\n",
        "\n",
        "# Realizar el primer cruce en X_test_df_prepro\n",
        "X_test_df_prepro = X_test_df_prepro.merge(OpportunityItemPostVenta_prepro_agg,\n",
        "                                            left_on=['OpportunityID'],\n",
        "                                            right_on=['OpportunityID'],\n",
        "                                            how='left')\n",
        "\n",
        "# Realizar el primer cruce en X_test_df_prepro\n",
        "X_test_df_prepro = X_test_df_prepro.merge(EquipmentPostVenta_prepro,\n",
        "                                          left_on=['CustomerID', 'EquipmentSerialNumber'],\n",
        "                                          right_on=['CurrentCustomerID', 'EquipmentID'],\n",
        "                                          how='left')\n",
        "\n",
        "# Realizar el segundo cruce en X_test_df_prepro\n",
        "X_test_df_prepro = X_test_df_prepro.merge(EquipmentPostVenta_prepro_agg,\n",
        "                                          left_on=['CustomerID'],\n",
        "                                          right_on=['CurrentCustomerID'],\n",
        "                                          how='left')\n",
        "\n",
        "# Realizar el cuarto cruce en X_test_df_prepro\n",
        "X_test_df_prepro = X_test_df_prepro.merge(horometros_vl_prepro_agg,\n",
        "                                            left_on=['EquipmentSerialNumber'],\n",
        "                                            right_on=['SerialNumber'],\n",
        "                                            how='left')\n",
        "\n",
        "# Eliminar las llaves redundantes de X_test_df_prepro\n",
        "X_test_df_prepro.drop(columns=['CurrentCustomerID_x', 'EquipmentID', 'CurrentCustomerID_y','SerialNumber','EquipmentSerialNumber','CustomerID'], inplace=True)"
      ],
      "metadata": {
        "id": "5ELWPu7ci45i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar los tipos de datos\n",
        "print(X_train_df_prepro.shape)\n",
        "print(X_train_df_prepro.dtypes.unique())\n",
        "print(X_train_df_prepro.dtypes[X_train_df_prepro.dtypes == 'O'])\n",
        "\n",
        "print(\"-------------------------\")\n",
        "\n",
        "print(X_test_df_prepro.shape)\n",
        "print(X_test_df_prepro.dtypes.unique())\n",
        "print(X_test_df_prepro.dtypes[X_test_df_prepro.dtypes == 'O'])"
      ],
      "metadata": {
        "id": "NZvYv0AZi47m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_df_prepro.drop(['EquipmentSerialNumber','CustomerID'],inplace=True)\n",
        "#X_test_df_prepro.drop(['EquipmentSerialNumber','CustomerID'],inplace=True)"
      ],
      "metadata": {
        "id": "R8pnmxEFi49s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compress_dset(\"X_train_df_prepro\", X_train_df_prepro)\n",
        "#compress_dset(\"X_test_df_prepro\", X_test_df_prepro)"
      ],
      "metadata": {
        "id": "NrJm2o-7i4_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_df_prepro.to_pickle('X_train_df_prepro.pkl')\n",
        "#X_test_df_prepro.to_pickle('X_test_df_prepro.pkl')"
      ],
      "metadata": {
        "id": "Rb2rRSevi5Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selección de variables"
      ],
      "metadata": {
        "id": "uGiAdr7RjBWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df_prepro = pd.read_pickle('X_train_df_prepro.pkl')\n",
        "X_test_df_prepro = pd.read_pickle('X_test_df_prepro.pkl')"
      ],
      "metadata": {
        "id": "qzPy1_x2i5Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list_objects_size()\n",
        "import gc\n",
        "#del OpportunityItemPostVenta_prepro_agg, horometros_vl_prepro_agg, EquipmentPostVenta_prepro, EquipmentPostVenta_prepro_agg\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "8qz7-lJgi5FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_df_prepro.shape)\n",
        "print(X_test_df_prepro.shape)"
      ],
      "metadata": {
        "id": "9agr7CBejHT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar los tipos de datos\n",
        "print(X_train_df_prepro.shape)\n",
        "print(X_train_df_prepro.dtypes.unique())\n",
        "print(X_train_df_prepro.dtypes[X_train_df_prepro.dtypes == 'O'])"
      ],
      "metadata": {
        "id": "i_7dX9lWjHWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validar los tipos de datos\n",
        "print(X_test_df_prepro.shape)\n",
        "print(X_test_df_prepro.dtypes.unique())\n",
        "print(X_test_df_prepro.dtypes[X_test_df_prepro.dtypes == 'O'])"
      ],
      "metadata": {
        "id": "_TNdBeOojHYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selección de variables (1era Iteración)\n",
        "#features_selected_init = feature_selection_customized_initial('OpportunityID', X_train_df_prepro, variance_threshold=0.01, max_missing_percentage=0.5, correlation_threshold=0.8)\n",
        "#len(features_selected_init)"
      ],
      "metadata": {
        "id": "4lqFM4qxjHaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selección de variables (1era Iteración)\n",
        "#features_selected_init = feature_selection_customized_last('OpportunityID', X_train_df_prepro, variance_threshold=0.01, max_missing_percentage=0.8, correlation_threshold=0.9)\n",
        "#len(features_selected_init)"
      ],
      "metadata": {
        "id": "OroF_g3jjHcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobar si se encuentra el target entre las variables seleccionadas"
      ],
      "metadata": {
        "id": "XrNZbwoQjQKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('ClosedWonOpportunity' in features_selected_init)"
      ],
      "metadata": {
        "id": "VZiYQ7rQjRQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selección de variables con RFE y LightGBM"
      ],
      "metadata": {
        "id": "AkEpcUhWjTka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\n",
        "# Función de transformación logarítmica\n",
        "def log_transform(x):\n",
        "    return np.log1p(x)\n",
        "\n",
        "# Ajustar conjuntos de datos de entrenamiento\n",
        "X_train_df_prepro = X_train_df_prepro[list(features_selected_init)]\n",
        "\n",
        "# Imputar nulos con -99999 aquellos nulos que no cruzaron en el cruce de las tablas\n",
        "X_train_df_prepro.fillna(-99999, inplace=True)\n",
        "\n",
        "# Separar variable objetivo\n",
        "y_train = X_train_df_prepro['ClosedWonOpportunity']\n",
        "X_train_df_prepro.drop(['ClosedWonOpportunity'], axis=1, inplace=True)\n",
        "X_train = X_train_df_prepro\n",
        "\n",
        "# Función para entrenamiento con LightGBM\n",
        "def train_lgb(X, y, params, num_boost_round=15):\n",
        "    X_train_df, X_valid_df, y_train_df, y_valid_df = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('log_transform', FunctionTransformer(log_transform, validate=False)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, X_train_df.columns)\n",
        "        ])\n",
        "\n",
        "    preprocessor.fit(X_train_df)\n",
        "    X_train_transformed = preprocessor.transform(X_train_df)\n",
        "    X_valid_transformed = preprocessor.transform(X_valid_df)\n",
        "\n",
        "    train_set = lgb.Dataset(X_train_transformed, label=y_train_df)\n",
        "    valid_set = lgb.Dataset(X_valid_transformed, label=y_valid_df, reference=train_set)\n",
        "\n",
        "    model = lgb.train(params, train_set, valid_sets=[train_set, valid_set], early_stopping_rounds=5, verbose_eval=False)\n",
        "    best_iteration = model.best_iteration\n",
        "    feature_importances = model.feature_importance()\n",
        "\n",
        "    return model, best_iteration, feature_importances\n",
        "\n",
        "# Función de eliminación recursiva de características\n",
        "def recursive_feature_elimination(X, y, params, threshold=0.05):\n",
        "    remaining_features = list(X.columns)\n",
        "    feature_importance_history = []\n",
        "\n",
        "    while len(remaining_features) > 1:\n",
        "        model, best_iteration, feature_importances = train_lgb(X[remaining_features], y, params)\n",
        "        feature_importance_history.append((remaining_features.copy(), feature_importances))\n",
        "\n",
        "        sorted_indices = np.argsort(feature_importances)\n",
        "        num_features_to_drop = max(1, int(len(remaining_features) * threshold))\n",
        "        features_to_drop = [remaining_features[i] for i in sorted_indices[:num_features_to_drop]]\n",
        "\n",
        "        for feature in features_to_drop:\n",
        "            remaining_features.remove(feature)\n",
        "\n",
        "        print(f\"Features dropped: {features_to_drop}\")\n",
        "        print(f\"Remaining features: {len(remaining_features)}\")\n",
        "\n",
        "    return feature_importance_history, remaining_features\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9\n",
        "}\n",
        "\n",
        "# Ejemplo de uso de la eliminación recursiva de características\n",
        "feature_importance_history, final_features = recursive_feature_elimination(X_train, y_train, params)\n",
        "\n",
        "print(f\"Final selected features: {final_features}\")"
      ],
      "metadata": {
        "id": "mhA3X_EwjYcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelado\n"
      ],
      "metadata": {
        "id": "4IBy1riujdGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def mean_target_encoding(train_df, test_df, target_col, cat_cols, n_splits=5):\n",
        "    # Create a copy of the dataframes to avoid modifying the original ones\n",
        "    train_df = train_df.copy()\n",
        "    test_df = test_df.copy()\n",
        "\n",
        "    # Initialize the KFold object\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Create a dictionary to hold the mean target values for each categorical column\n",
        "    mean_encoded_cols = {col: f'{col}_mean_encoded' for col in cat_cols}\n",
        "\n",
        "    # Add new columns for mean target encoding to the train and test dataframes\n",
        "    for col in cat_cols:\n",
        "        train_df[mean_encoded_cols[col]] = np.nan\n",
        "        test_df[mean_encoded_cols[col]] = 0\n",
        "\n",
        "    # Perform mean target encoding using KFold cross-validation\n",
        "    for train_index, val_index in kf.split(train_df):\n",
        "        X_train, X_val = train_df.iloc[train_index], train_df.iloc[val_index]\n",
        "\n",
        "        for col in cat_cols:\n",
        "            means = X_train.groupby(col)[target_col].mean()\n",
        "            train_df.loc[val_index, mean_encoded_cols[col]] = X_val[col].map(means)\n",
        "\n",
        "    # Calculate the overall mean for each categorical column and apply it to the test set\n",
        "    for col in cat_cols:\n",
        "        overall_means = train_df.groupby(col)[target_col].mean()\n",
        "        test_df[mean_encoded_cols[col]] = test_df[col].map(overall_means)\n",
        "\n",
        "        # For any missing values in the mean encoded columns, fill with the overall mean of the target\n",
        "        train_df[mean_encoded_cols[col]].fillna(train_df[target_col].mean(), inplace=True)\n",
        "        test_df[mean_encoded_cols[col]].fillna(train_df[target_col].mean(), inplace=True)\n",
        "\n",
        "    # Drop the original categorical columns\n",
        "    #train_df.drop(columns=cat_cols, inplace=True)\n",
        "    #test_df.drop(columns=cat_cols, inplace=True)\n",
        "\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "id": "eT3gmIP_jYeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with your data\n",
        "cat_cols = X_train.select_dtypes(include=['object']).columns\n",
        "target_col = 'ClosedWonOpportunity'\n",
        "\n",
        "X_train, X_test = mean_target_encoding(pd.concat([X_train, y_train], axis=1), X_test, target_col, cat_cols)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "ydNr5A2DjYgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from category_encoders import TargetEncoder\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "# Define the model with parameter flexibility\n",
        "def get_model(n_estimators=100, random_state=42):\n",
        "    return lgb.LGBMClassifier(n_estimators=n_estimators, random_state=random_state)\n",
        "\n",
        "# Function to apply log transformation to skewed numerical features\n",
        "def log_transform(x):\n",
        "    return np.log1p(x)\n",
        "\n",
        "# Prepare the training and testing datasets\n",
        "# Assuming X_train and X_test are already defined DataFrames and y_train is the target Series\n",
        "\n",
        "# Identify the identifier column\n",
        "identifier_col = 'OpportunityID'\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object' and col != identifier_col]\n",
        "numerical_cols = [col for col in X_train.columns if X_train[col].dtype != 'object' and col != identifier_col]\n",
        "\n",
        "# One-Hot Encoding for low cardinality\n",
        "low_cardinality_cols = [col for col in categorical_cols if X_train[col].nunique() < 10]\n",
        "high_cardinality_cols = list(set(categorical_cols) - set(low_cardinality_cols))\n",
        "\n",
        "# Preprocessing for numerical data\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('log_transform', FunctionTransformer(log_transform, validate=False)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for low cardinality categorical data\n",
        "low_cardinality_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Preprocessing for high cardinality categorical data\n",
        "high_cardinality_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('target', TargetEncoder())\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('low_cat', low_cardinality_transformer, low_cardinality_cols),\n",
        "        ('high_cat', high_cardinality_transformer, high_cardinality_cols)\n",
        "    ])\n",
        "\n",
        "# Define the model\n",
        "model = get_model()\n",
        "\n",
        "# Bundle preprocessing and modeling code in a pipeline\n",
        "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('model', model)\n",
        "                     ])\n",
        "\n",
        "# 5-fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "custom_scores = []\n",
        "\n",
        "print(\"Starting 5-fold cross-validation...\")\n",
        "\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(X_train, y_train), 1):\n",
        "    print(f\"Processing fold {fold}...\")\n",
        "\n",
        "    # Selecting features excluding the identifier column\n",
        "    X_train_split = X_train.iloc[train_index].drop(columns=[identifier_col])\n",
        "    X_valid_split = X_train.iloc[valid_index].drop(columns=[identifier_col])\n",
        "    y_train_split = y_train.iloc[train_index]\n",
        "    y_valid_split = y_train.iloc[valid_index]\n",
        "\n",
        "    # Fit model\n",
        "    clf.fit(X_train_split, y_train_split)\n",
        "    print(f\"Fold {fold}: Model training completed.\")\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = clf.predict_proba(X_valid_split)[:, 1]\n",
        "    y_valid_df = pd.DataFrame({'ClosedWonOpportunity': y_valid_split})\n",
        "    y_pred_df = pd.DataFrame({'ClosedWonOpportunity': y_pred})\n",
        "\n",
        "    # Calculate custom metric\n",
        "    score = custom_metric(y_valid_df, y_pred_df)\n",
        "    custom_scores.append(score)\n",
        "    print(f\"Calculating score for fold {fold}, score: {score}\")\n",
        "\n",
        "# Calculate and print the average custom metric score across the 5 folds\n",
        "average_custom_score = np.mean(custom_scores)\n",
        "print(f'Average Custom Metric Score across 5 folds: {average_custom_score:.3f}')\n",
        "\n",
        "# Fit the final model on the entire training set excluding the identifier column\n",
        "clf.fit(X_train.drop(columns=[identifier_col]), y_train)\n",
        "\n",
        "# Make predictions on the test set excluding the identifier column\n",
        "y_test_pred = clf.predict_proba(X_test.drop(columns=[identifier_col]))[:, 1]"
      ],
      "metadata": {
        "id": "HgT7LMjnjYjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper parameter tuning"
      ],
      "metadata": {
        "id": "RSRsAww2g47A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "import lightgbm as lgb\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, r2_score\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define log transform function\n",
        "def log_transform(x):\n",
        "    return np.log1p(x)\n",
        "\n",
        "# Custom metric function\n",
        "def custom_metric(solution: pd.DataFrame, submission: pd.DataFrame,\n",
        "                  solution_target_col_name='ClosedWonOpportunity', submission_pred_col_name='ClosedWonOpportunity'):\n",
        "    # Initialize DataFrame for metrics\n",
        "    target = 'target'\n",
        "    pred = 'pred'\n",
        "    metric = pd.DataFrame()\n",
        "    metric[target] = solution[solution_target_col_name].reset_index(drop=True)\n",
        "    metric[pred] = submission[submission_pred_col_name].reset_index(drop=True)\n",
        "\n",
        "    # Calculate deciles and metrics\n",
        "    decile_labels = [f'D{i:02}' for i in range(10, 0, -1)]  # From D10 to D01\n",
        "    metric['decil'] = pd.qcut(metric[pred], q=10, labels=decile_labels)\n",
        "\n",
        "    # Cumulative target by decile\n",
        "    cumulative_target_by_decile = (metric.groupby('decil')[target].sum() / metric[target].sum()).sort_index(ascending=False).cumsum()\n",
        "\n",
        "    # Decile counts and best decile score\n",
        "    decile_counts = metric['decil'].value_counts().sort_index(ascending=True)\n",
        "    total_target = metric[target].sum()\n",
        "    best_decil_score = (decile_counts / total_target).mean().round(4)\n",
        "\n",
        "    # Create the perfect curve\n",
        "    perfect_curve = np.cumsum([best_decil_score] * len(decile_labels))\n",
        "    perfect_curve[perfect_curve > 1] = 1  # Cap values at 1\n",
        "\n",
        "    # Calculate AUC of the perfect model\n",
        "    decile_labels_scaled = np.linspace(0, 1, len(decile_labels))\n",
        "    PAUC_max = np.trapz(perfect_curve, decile_labels_scaled)\n",
        "\n",
        "    # Calculate Prioritization AUC\n",
        "    priorization_auc = np.trapz(cumulative_target_by_decile, decile_labels_scaled) / PAUC_max\n",
        "\n",
        "    # ROC AUC and Calibration Curve R2\n",
        "    roc_auc = roc_auc_score(metric[target], metric[pred])\n",
        "    fpr, tpr, thresholds = roc_curve(metric[target], metric[pred])\n",
        "    prob_true, prob_pred = calibration_curve(metric[target], metric[pred], n_bins=20, strategy='uniform')\n",
        "    r2 = r2_score(np.linspace(0, 1, len(prob_true)), prob_true)\n",
        "\n",
        "    # Final combined metric\n",
        "    final_metric = 0.5 * priorization_auc + 0.3 * roc_auc + 0.2 * r2\n",
        "\n",
        "    return final_metric\n",
        "\n",
        "# Define objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Hyperparameter space\n",
        "    param = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 500),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 10.0),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 10.0),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
        "        'num_boost_round': 100\n",
        "    }\n",
        "\n",
        "    # Split the data\n",
        "    X_train_df, X_valid_df, y_train_df, y_valid_df = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
        "\n",
        "    # Preprocessing steps\n",
        "    identifier_col = 'OpporunityID'  # Assuming there's an identifier column named 'ID'\n",
        "    categorical_cols = [col for col in X_train_df.columns if X_train_df[col].dtype == 'object' and col != identifier_col]\n",
        "    numerical_cols = [col for col in X_train_df.columns if X_train_df[col].dtype != 'object' and col != identifier_col]\n",
        "\n",
        "    # One-Hot Encoding for low cardinality\n",
        "    low_cardinality_cols = [col for col in categorical_cols if X_train_df[col].nunique() < 10]\n",
        "    high_cardinality_cols = list(set(categorical_cols) - set(low_cardinality_cols))\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('log_transform', FunctionTransformer(log_transform, validate=False)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Preprocessing for low cardinality categorical data\n",
        "    low_cardinality_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Preprocessing for high cardinality categorical data\n",
        "    high_cardinality_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('target', TargetEncoder())\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('low_cat', low_cardinality_transformer, low_cardinality_cols),\n",
        "            ('high_cat', high_cardinality_transformer, high_cardinality_cols)\n",
        "        ])\n",
        "\n",
        "    # Define the model\n",
        "    model = lgb.LGBMClassifier(**param)\n",
        "\n",
        "    # Bundle preprocessing and modeling code in a pipeline\n",
        "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                          ('model', model)\n",
        "                         ])\n",
        "\n",
        "    # Fit model\n",
        "    clf.fit(X_train_df, y_train_df)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = clf.predict_proba(X_valid_df)[:, 1]\n",
        "\n",
        "    # Calculate custom metric\n",
        "    solution = pd.DataFrame({'ClosedWonOpportunity': y_valid_df})\n",
        "    submission = pd.DataFrame({'ClosedWonOpportunity': y_pred})\n",
        "    custom_metric_score = custom_metric(solution, submission)\n",
        "\n",
        "    return custom_metric_score\n",
        "\n",
        "\n",
        "# Optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Print best trial\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(trial.values)\n",
        "print(trial.params)"
      ],
      "metadata": {
        "id": "WY16GCzsg7gu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}